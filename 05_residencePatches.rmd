---
editor_options: 
  chunk_output_type: console
---

# Residence patch construction

This section is about using the main `watlasUtils` functions to infer residence points when data is missing from a movement track, to classify points into residence or travelling, and to construct low-tide residence patches from the residence points. Summary statistics on these spatial outputs are then exported to file for further use.

**Workflow**

1. Prepare `watlasUtils` and required libraries,
2. Read data, infer residence, classify points, construct low-tide patches, and get spatial data from patches,
3. Get patch trajectories,
4. Export spatial data.

## Prepare libraries

```{r prep_libs, message=FALSE, warning=FALSE}
# load watlasUtils or install if not available
if("watlasUtils" %in% installed.packages() == FALSE){
  devtools::install_github("pratikunterwegs/watlasUtils")
}
library(watlasUtils)

# libraries to process data
library(data.table)
library(purrr)
library(stringr)
library(glue)
library(readr)
library(dplyr)
library(fasttime)

# libraries for the cluster
library(ssh)
```

## Patch construction

Connect to cluster and process data up to and beyond patches.

```{r make_patches, eval=FALSE, message=FALSE, warning=FALSE}
# read password
password = read_csv("data/password.txt")$password

# get bird ids
data_ids <- str_extract(data_files, "(tx_\\d+)") %>% str_sub(-3,-1)

# make a list of data files to read
data_files <- list.files(path = "data/data2018/revisitData", pattern = "_revisit.csv", full.names = TRUE)

# map inferResidence, classifyPath, and getPatches over data

output_data <- map(data_files, function(df){

  # connect to peregrine and transfer data
  {
    s <- ssh_connect("p284074@peregrine.hpc.rug.nl", passwd = password)
    # make directory if non-existent
    ssh_exec_wait(s, command = "mkdir -p data/watlas_2018")
    
    # list files already present
    files_on_prg <- ssh_exec_internal(s, command = "ls data/watlas_2018")
    files_on_prg <- rawToChar(files_on_prg$stdout) %>% 
      str_split("\n") %>% 
      unlist()
    
    # check name
    data_name <- df %>% 
      str_split("/") %>% 
      unlist() %>% .[3]
    
    if(!data_name %in% files_on_prg){
      # upload data file for processing
      scp_upload(s, df, to = "data/watlas_2018/")
    }
  }

  # get id tide combination
  id_tide <- as.character(glue('{unique(df$id)}_{unique(df$tide_number)}'))

  # make job file
  {
    shebang <- readLines("code/template_job_patches.sh")
    
    # rename job
    shebang[2] <- glue('#SBATCH --job-name=patches_{id_tide}')
    
    text <- glue('Rscript code/do_patches_2018.r {df}')
    jobfile <- glue('code/job_patches_{id_tide}.sh')
    
    writeLines(c(shebang, text), con = jobfile)
    scp_upload(s, jobfile, to = "code/")
    
    # remove jobfile
    file.remove(jobfile)
  }
  
  ssh_exec_wait(s, command = glue('dos2unix {jobfile}'))
  # process using ctmm
  ssh_exec_wait(s, command = glue('sbatch {jobfile}'))
  
  # disconnect
  ssh_disconnect(s)
})

```

## Get patch trajectories

```{r patch_trajectories, eval=FALSE, message=FALSE, warning=FALSE}
# save as temp data
save(output_data, file = "data/data2018/patch_data_2018.rdata")
# filter non-sf objects from the list
output_data <- keep(output_data, function(obj){"data.frame" %in% class(obj)})
output_data <- bind_rows(output_data)

fwrite(output_data, file = "data/data2018/patch_summary.csv",
       dateTimeAs = "ISO")
```

