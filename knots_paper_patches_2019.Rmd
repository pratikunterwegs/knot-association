--- 
knit: "bookdown::render_book"
title: "Individual consistency in space-use across scales"
author: "Pratik R Gupte"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [refs.bib]
biblio-style: apalike
link-citations: yes
github-repo: pratikunterwegs/knotPatches
description: "Individual consistency in space-use across scales"
---

# Introduction

This is the `bookdown` version of a project in preparation that links high-resolution tracking data from individual red knots _Calidris canutus islandica_ to fine-scale experimental behaviour measurements in captivity, and examines whether individuals are consistent across scales in their space-use.

## Attribution

Please contact the following before cloning or in case of interest in the project.

- Pratik Gupte (author and maintainer)
  - [PhD student, GELIFES -- University of Groningen](https://www.rug.nl/staff/p.r.gupte)
  - Guest researcher, COS -- NIOZ
  - p.r.gupte@rug.nl
  - Nijenborgh 7/5172.0583 9747AG Groningen

- Allert Bijleveld (PI): allert.bijleveld@nioz.nl
  - Project information: https://www.nioz.nl/en/about/cos/coastal-movement-ecology/shorebird-tracking

- Selin Ersoy (collab): selin.ersoy@nioz.nl

## Data access

The data used in this work are not publicly available. Contact PI Allert Bijleveld for data access.

## Data processing

The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar.

<!--chapter:end:index.rmd-->

---
editor_options: 
  chunk_output_type: console
---

# Getting data

This section focusses on accessing and downloading WATLAS data. This is done using functions in the [WATLAS Utilities package](https://github.com/pratikunterwegs/watlastools). 

**Workflow**

1. Preparing required libraries.
2. Reading tag data with deployment start dates from a local file. This file is not yet publicly available.
3. Connecting to the NIOZ databse and downloading data. This database is also not public-access.

## Prepare `watlastools` and other libraries

```{r install_watlastools, message=FALSE, warning=FALSE}
# install the package watlastools from master branch using the following
# install.packages("devtools")
library(devtools)

# devtools::install_github("pratikunterwegs/watlastools")
library(watlastools)

# libraries to process data
library(data.table)
library(ggplot2)
library(ggthemes)
library(purrr)
library(glue)
```

## Read in tag deployment data

```{r get_deployment_data, message=FALSE, warning=FALSE}
# read deployment data from local file in data folder
tag_info <- fread("data/data2018/SelinDB.csv")

# filter out NAs in release date and time
tag_info <- tag_info[!is.na(Release_Date) & !is.na(Release_Time),]

# make release date column as POSIXct
tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = " "), format = "%d.%m.%y %H:%M", tz = "CET")]

# check new release date column
head(tag_info$Release_Date)
```

```{r plot_release_schedule, echo=FALSE, fig.cap="Knots released per week of 2018.", message=FALSE, warning=FALSE}
# check release cohort
ggplot(tag_info)+
  geom_bar(aes(x = week(Release_Date)), col = 1, fill = "grey")+
  ggthemes::theme_few()+
  labs(x = "release week (2018)", y = "# knots released",
       caption = Sys.time())
```


## Get data and save locally

```{r get_acess, warning=FALSE, message=FALSE}
# read in database access parameters from a local file
data_access <- fread("data/access_params.txt")
```

```{r get_data, eval=FALSE, message=FALSE, warning=FALSE}
# create a data storage file if not present
# use the getData function from watlastools on the tag_info data frame
# this is placed inside a pmap wrapper to automate access for all birds

if(!dir.exists("data/data2018")) { dir.create("data/data2018") }

pmap(tag_info[,.(Toa_Tag, Release_Date)], function(Toa_Tag, Release_Date){

  prelim_data <- watlastools::wat_get_data(tag = Toa_Tag,
                           tracking_time_start = as.character(Release_Date),
                           tracking_time_end = "2018-10-31",
                           username = data_access$username,
                           password = data_access$password)
  
  setDT(prelim_data)
  # prelim_data[,TAG:= = as.numeric(TAG) - prefix_num]

  message(glue('tag {Toa_Tag} accessed with {nrow(prelim_data)} fixes'))
  
  fwrite(prelim_data, file = glue('data/data2018/{Toa_Tag}_data.csv'),
         dateTimeAs = "ISO")
  
})
```


<!--chapter:end:01_gettingData.rmd-->

---
editor_options: 
  chunk_output_type: console
---

# Cleaning data

This section is about cleaning downloaded data using the `cleanData` function in the [WATLAS Utilities package](https://github.com/pratikunterwegs/watlastools).

**Workflow**

1. Prepare required libraries.
2. Read in data, apply the cleaning function, and overwrite local data.

## Prepare `watlastools` and other libraries

```{r install_watlastools_2, message=FALSE, warning=FALSE, eval=FALSE}
# watlastools assumed installed from the previous step
# if not, install from the github repo as shown below

devtools::install_github("pratikunterwegs/watlastools")
library(watlastools)

# libraries to process data
library(data.table)
library(purrr)
library(glue)
library(fasttime)
library(bit64)
library(stringr)
```

## Prepare to remove attractor points

```{r read_attractors, eval=FALSE, message=FALSE, warning=FALSE}
# read in identified attractor points
atp <- fread("data/data2018/attractor_points.txt")
```

## Read, clean, and write data

```{r read_in_raw_data, eval=FALSE, message=FALSE, warning=FALSE}
# make a list of data files to read
data_files <- list.files(path = "data/data2018/locs_raw", pattern = "whole_season*", full.names = TRUE)

data_ids <- str_extract(data_files, "(tx_\\d+)") %>% str_sub(-3,-1)

# read deployment data from local file in data folder
tag_info <- fread("data/data2018/SelinDB.csv")

# filter out NAs in release date and time
tag_info <- tag_info[!is.na(Release_Date) & !is.na(Release_Time),]

# make release date column as POSIXct
tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = " "), format = "%d.%m.%y %H:%M", tz = "CET")]

# sub for knots in data
data_files <- data_files[as.integer(data_ids) %in% tag_info$Toa_Tag]

# map read in, cleaning, and write out function over vector of filenames
map(data_files, function(df){
  
  temp_data <- fread(df, integer64 = "numeric")
  
  # filter for release date + 24 hrs
  {
    temp_id <- str_sub(temp_data[1, TAG], -3, -1)
    
    rel_date <- tag_info[Toa_Tag == temp_id, Release_Date]
    
    temp_data <- temp_data[TIME/1e3 > as.numeric(rel_date + (24*3600)),]
  }
  tryCatch(
    {
      temp_data <- wat_rm_attractor(df = temp_data,
                                    atp_xmin = atp$xmin,
                                    atp_xmax = atp$xmax,
                                    atp_ymin = atp$ymin,
                                    atp_ymax = atp$ymax)
      
      clean_data <- wat_clean_data(somedata = temp_data,
                                   moving_window = 3,
                                   nbs_min = 0,
                                   sd_threshold = 100,
                                   filter_speed = TRUE,
                                   speed_cutoff = 150)
      
      agg_data <- wat_agg_data(df = clean_data,
                               interval = 30)
      
      message(glue('tag {unique(agg_data$id)} cleaned with {nrow(agg_data)} fixes'))
      
      fwrite(x = agg_data, file = glue('data/data2018/locs_proc/id_{temp_id}.csv'), dateTimeAs = "ISO")
      rm(temp_data, clean_data, agg_data)
      },
    error = function(e){
      message(glue('tag {unique(temp_id)} failed'))
    })
})

```


<!--chapter:end:02_cleaningData.rmd-->

---
editor_options: 
  chunk_output_type: console
---

# Adding tidal cycle data

This section is about adding tidal cycle data to individual trajectories. This is done to split the data up into convenient, and biologically sensible units. This section uses the package `VulnToolkit` [@VulnToolkit2014] to identify high tide times from water-level data provided by Rijkswaterstaat for the measuring point at West Terschelling.

**Workflow**

1. Prepare required libraries,
2. Read in water level data and identify high tides,
3. Write tidal cycle data to local file,
4. Add time since high tide to movement data.

## Prepare libraries

```{r install_vulntoolkit_2, message=FALSE, warning=FALSE, eval=FALSE}
# load VulnToolkit or install if not available
if("VulnToolkit" %in% installed.packages() == FALSE){
  devtools::install_github("troyhill/VulnToolkit")
}
library(VulnToolkit)

# libraries to process data
library(data.table)
library(purrr)
library(glue)
library(dplyr)
library(stringr)
library(fasttime)
library(lubridate)

library(watlastools)
```

## Read water level data

Water level data for [West Terschelling](https://waterinfo.rws.nl/#!/details/publiek/waterhoogte-t-o-v-nap/West-Terschelling(WTER)/Waterhoogte___20Oppervlaktewater___20t.o.v.___20Normaal___20Amsterdams___20Peil___20in___20cm), a settlement approx. 10km from the field site are provided by Rijkswaterstaat's [Waterinfo](https://waterinfo.rws.nl/#!/nav/bulkdownload/parameters/Waterhoogten/), in cm above Amsterdam Ordnance Datum. These data are manually downloaded in the range July 1, 2018 -- October 31, 2018 and saved in `data/data2018`.

```{r read_waterlevel_data, message=FALSE, warning=FALSE, eval=FALSE}
# read in waterlevel data
waterlevel <- fread("data/data2018/waterlevelWestTerschelling.csv", sep = ";")

# select useful columns and rename
waterlevel <- waterlevel[,.(WAARNEMINGDATUM, WAARNEMINGTIJD, NUMERIEKEWAARDE)]

setnames(waterlevel, c("date", "time", "level"))

# make a single POSIXct column of datetime
waterlevel[,dateTime := as.POSIXct(paste(date, time, sep = " "), 
                                   format = "%d-%m-%Y %H:%M:%S", tz = "CET")]

waterlevel <- setDT(distinct(setDF(waterlevel), dateTime, .keep_all = TRUE))
```

## Calculate high tides

A tidal period of 12 hours 25 minutes is taken from [Rijkswaterstaat](https://www.rijkswaterstaat.nl/water/waterdata-en-waterberichtgeving/waterdata/getij/index.aspx).

```{r get_high_tide, eval=FALSE, message=FALSE, warning=FALSE}
# use the HL function from vulnToolkit to get high tides
tides <- VulnToolkit::HL(waterlevel$level,
                         waterlevel$dateTime, period = 12.41, 
                         tides = "H", semidiurnal = TRUE)

# read in release data and get first release - 24 hrs
tag_info <- fread("data/data2018/SelinDB.csv")
tag_info <- tag_info[!is.na(Release_Date) & !is.na(Release_Time),]
tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = " "), format = "%d.%m.%y %H:%M", tz = "CET")]

first_release <- min(tag_info$Release_Date) - (3600*24)

# remove tides before first release
tides <- setDT(tides)[time > first_release, ][,tide2:=NULL]
tides[,tide_number:=1:nrow(tides)]
```

```{r write_tide_data, eval=FALSE, message=FALSE, warning=FALSE}
# write to local file
fwrite(tides, file = "data/data2018/tidesSummer2018.csv", 
       dateTimeAs = "ISO")
```

## Add time since high tide

```{r time_to_high_tide, eval=FALSE, message=FALSE, warning=FALSE}
# read in data and add time since high tide
data_files <- list.files(path = "data/data2018/locs_proc", pattern = "id_", full.names = TRUE)
data_ids <- str_extract(data_files, "(id_\\d+)") %>% str_sub(-3,-1)

# map read in and tidal time calculation over data
# merge data to insert high tides within movement data
# arrange by time to position high tides correctly
map(data_files, function(df){
  
  # read and fix data types
  temp_data <- fread(df, integer64 = "numeric")
  temp_data[,ts:=fastPOSIXct(ts, tz = "CET")]
  
  # merge with tides and order on time
  temp_data <- wat_add_tide(df = temp_data,
               tide_data = "data/data2018/tidesSummer2018.csv")

  # add waterlevel
  temp_data[,temp_time := lubridate::round_date(ts, unit = "10 minute")]
  temp_data <- merge(temp_data, waterlevel[,.(dateTime, level)], 
                     by.x = "temp_time", by.y = "dateTime")
  setnames(temp_data, old = "level", new = "waterlevel")
  
  # export data, print msg, remove data
  fwrite(temp_data, file = df, dateTimeAs = "ISO")
  message(glue('tag {unique(temp_data$id)} added time since high tide'))
  rm(temp_data)
})
```


<!--chapter:end:03_addingTides.rmd-->

---
editor_options: 
  chunk_output_type: console
---

# Knots in modules

## Prepare libraries

```{r prep_libs_03, eval=FALSE}
library(tidyverse)

# for plots
library(ggplot2)
library(plotly)
library(htmlwidgets)


# ci function
ci <- function(x){qnorm(0.975)*sd(x, na.rm = TRUE)/sqrt(length(x))}
```

## Read in data for small scale

```{r read_modules, eval=FALSE}
# read in data
modules <- read_csv("data/data_2018_patch_modules_small_scale.csv")
data <- read_csv("data/data_2018_good_patches.csv")
```

Choose the smallest spatial scale.

```{r choose_scale, eval=FALSE}
modules <- filter(modules, spatial_scale <= 50, time_scale == 1)
```

## Add patch data

```{r add_patch_data, eval=FALSE}
# get mean and ci for patches
mod_data <- modules %>% 
  inner_join(data)

# count unique ids in each mod-time-chunk and filter on 2
mod_data <- mod_data %>% 
  group_by(spatial_scale, module, time_chunk) %>% 
  mutate(xc = mean(x_mean, na.rm = T),
         yc = mean(y_mean, na.rm = T),
         wlc = mean(waterlevel_start),
         wlc = plyr::round_any(wlc, 25),
         n_uid = length(unique(id)),
         n_patches = length(unique(x_mean)))
```

Filter for minimum 2 indiviudals per module, and minimum of 3 patches.

```{r filter_n_id}
mod_data <- filter(mod_data, n_uid > 1, n_patches > 3)
```

## Visualise modules

```{r vis_modules, eval=FALSE}
# select single time chunk
a = ggplot(mod_data)+
  geom_segment(aes(x_mean, y_mean,
                xend = xc, yend = yc),
               size = 0.1,
               show.legend = FALSE)+
  geom_point(aes(x_mean, y_mean,
             col = time_chunk,
             fill = duration/60),
             stroke = 0.1,
             shape = 21,
             show.legend = FALSE,
             alpha = 0.2)+
  scale_colour_distiller("Greys")+
  scale_fill_distiller(palette = "YlOrRd")+
  coord_sf(crs = 32631)+
  # facet_wrap(~time_chunk)+
  theme(axis.text = element_blank())


figplotly <- ggplotly(a, tooltip = c("colour", "fill"))

saveWidget(figplotly, file = "fig_modules.html")

ggsave(a, filename = "figs/fig_modules.png", dpi = 300,
       width = 24, height = 25)
```


## Module size and composition ~ waterlevel

How does the number of patches and number of ids change with waterlevel?

```{r module_size_waterlevel, eval=FALSE}
# prep data to do stuff
mods_waterlevel <- mod_data %>% 
  ungroup() %>% 
  pivot_longer(cols = c("n_patches", "n_uid")) %>% 
  group_by(wlc, name) %>% 
  summarise_at(vars(value),
               list(~mean(., na.rm = T),
                    ~ci(.)))

# svae fig
fig_mods_waterlevel <- 
  ggplot(mods_waterlevel,
       aes(wlc, mean, 
           ymin=mean-ci, ymax=mean+ci))+
  geom_pointrange(size = 0.2, col = "steelblue")+
  theme_test()+
  facet_grid(~name, as.table = F)+
  labs(x = "waterlevel (cm NAP)",
       y = "value")

ggsave(fig_mods_waterlevel, 
       filename = "figs/fig_mods_waterlevel.png",
       device = png(), dpi = 300)
```

```{r}
knitr::include_graphics("figs/fig_mods_waterlevel.png")
```


## Patch size ~ number of patches in a module

```{r patch_size_module, eval=FALSE}
# count patches in module, and get mean patch area, duration, circ
# distance between patches
patch_per_mod <- modules %>% 
  group_by(time_scale, spatial_scale, time_chunk, module) %>% 
  summarise(n_patches = length(patch),
            n_uid = length(unique(id)))


# summarise data
mod_data <- mod_data %>% 
  group_by(spatial_scale, time_chunk, module) %>% 
  summarise_at(vars(area, duration, circularity, distBwPatch),
               list(~mean(., na.rm = T)))

# join with patch per mod
mod_data <- inner_join(mod_data, patch_per_mod)

# remove data above the 95% percentile
quantile(mod_data$n_patches, probs = c(0.05, 0.90))

# get by n patch
mod_summary <- mod_data %>% 
  filter(n_patches <= 20)  %>% 
  pivot_longer(cols = c("area", "duration", 
                        "circularity", "distBwPatch")) %>% 
  drop_na() %>% 
  mutate(round_n_patches = plyr::round_any(n_patches, 1)) %>% 
  group_by(time_scale, spatial_scale, round_n_patches, name) %>% 
  summarise_at(vars(value),
               list(~mean(., na.rm = T),
                    ~ci(.)))

# make list
mod_summary <- split(mod_summary, mod_summary$name)
```

```{r plot_metrics_size, eval=FALSE}
# do plot over list

list_plot <- map2(mod_summary, 
                  letters[1:4],
                  function(df, name){
  ggplot(df)+
    geom_pointrange(aes(round_n_patches, mean,
                        ymin = mean - ci, ymax = mean+ci),
                    col = "steelblue",
                    size = 0.1)+
    theme_test(base_size = 8)+
    facet_grid(spatial_scale~time_scale,
               labeller = label_both,
               # scales = "free_y",
               as.table = FALSE,
               switch = "both")+
    labs(x = "# patches",
         y = glue::glue('{unique(df$name)}'),
         title = glue::glue('({name})'))  
})

fig_metrics_social <- patchwork::wrap_plots(list_plot)

ggsave(fig_metrics_social, filename = "figs/fig_metrics_social.png",
       dpi = 300)
```

```{r}
knitr::include_graphics("figs/fig_metrics_social.png")
```


<!--chapter:end:03_patch_size_in_modules.rmd-->

---
editor_options: 
  chunk_output_type: console
---

# Revisit analysis

This section is about splitting the data by tidal cycle, and passing the individual- and tidal cycle-specific data to revisit analysis, which is implemented using the package `recurse`.

**Workflow**

1. Prepare required libraries,
2. Performing recurse:
  - Read in movement data and split by tidal cycle,
  - Perform revisit analysis using `recurse`,
  - Write data with revisit metrics to file.

## Prepare libraries

This section uses the recurse package [@bracis2018].

```{r install_recurse, message=FALSE, warning=FALSE, eval=FALSE}
# load recurse or install if not available
if("recurse" %in% installed.packages() == FALSE){
  install.packages("recurse")
}
library(recurse)

# libraries to process data
library(data.table)
library(purrr)
library(glue)
library(dplyr)
library(fasttime)
library(stringr)
```

## Read data, split, recurse, write

```{r recurse_analysis, eval=FALSE, message=FALSE, warning=FALSE}
# read in data
data_files <- list.files(path = "data/data2018/", pattern = "id_", full.names = TRUE)

data_ids <- str_extract(data_files, "(id_\\d+)") %>% str_sub(-3,-1)

{
  # read in release data and get first release - 24 hrs
  tag_info <- fread("data/data2018/SelinDB.csv")
  tag_info <- tag_info[!is.na(Release_Date) & !is.na(Release_Time),]
  tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = " "), format = "%d.%m.%y %H:%M", tz = "CET")]
}

# prepare recurse data folder
if(!dir.exists("data/data2018/revisitData")){ dir.create("data/data2018/revisitData") }


# prepare recurse in parameters
# radius (m), cutoff (mins)
{
  radius <- 50
  timeunits <- "mins"
  revisit_cutoff <- 60
}

# map read in, splitting, and recurse over individual level data
# remove visits where the bird left for 60 mins, and then returned
# this is regardless of whether after its return it stayed there
# the removal counts the cumulative sum of all (timeSinceLastVisit <= 60)
# thus after the first 60 minute absence, all points are assigned TRUE
# this must be grouped by the coordinate
map(data_files, function(df){
  
  # read in, fix data type, and split
  temp_data <- fread(df, integer64 = "numeric")
  temp_data[,ts:=fastPOSIXct(ts, tz = 'CET')]
  setDF(temp_data)
  temp_data <- split(temp_data, temp_data$tide_number)
  
  # map over the tidal cycle level data
  map(temp_data, function(tempdf){
    
    tryCatch({
    # perform the recursion analysis
    df_recurse <- getRecursions(x = tempdf[,c("x","y","ts","id")], 
                                radius = radius, 
                                timeunits = timeunits, verbose = TRUE)
    
    # extract revisit statistics and calculate residence time
    # and revisits with a 1 hour cutoff
    df_recurse <- setDT(df_recurse[["revisitStats"]])
    
    df_recurse[,timeSinceLastVisit:= 
                 ifelse(is.na(timeSinceLastVisit), -Inf, timeSinceLastVisit)]
    
    df_recurse[,longAbsenceCounter:= cumsum(timeSinceLastVisit > 60), 
               by= .(coordIdx)]
    
    df_recurse <- df_recurse[longAbsenceCounter < 1,]
    
    df_recurse <- df_recurse[,.(resTime = sum(timeInside), 
                                fpt = first(timeInside),
                                revisits = max(visitIdx)),
                             by=.(coordIdx,x,y)]
    
    # prepare and merge existing data with recursion data
    setDT(tempdf)[,coordIdx:=1:nrow(tempdf)]
    
    tempdf <- merge(tempdf, df_recurse, by = c("x", "y", "coordIdx"))
    
    setorder(tempdf, ts)
    
    # write each data frame to file
    fwrite(tempdf, 
           file = glue('data/data2018/revisitData/{unique(tempdf$id)}_{str_pad(unique(tempdf$tide_number), width=3, pad="0")}_revisit.csv'))
    
    message(glue('recurse {unique(tempdf$id)}_{str_pad(unique(tempdf$tide_number), width=3, pad="0")} done'))
    
    rm(tempdf, df_recurse)
    },
    error = function(e){
      message("some recurses failed")
    })
    
  })
  
})
```


<!--chapter:end:04_revisitAnalysis.rmd-->

---
editor_options: 
  chunk_output_type: console
---

# Residence patch construction

This section is about using the main `watlastools` functions to infer residence points when data is missing from a movement track, to classify points into residence or travelling, and to construct low-tide residence patches from the residence points. Summary statistics on these spatial outputs are then exported to file for further use.

**Workflow**

1. Prepare `watlastools` and required libraries,
2. Read data, infer residence, classify points, construct patches, repair patches, and write movement data and patch summary to file.

## Prepare libraries

```{r prep_libs, message=FALSE, warning=FALSE, eval=FALSE}
# load watlastools or install if not available
if("watlastools" %in% installed.packages() == FALSE){
  devtools::install_github("pratikunterwegs/watlastools")
}
library(watlastools)

# libraries to process data
library(dplyr)
library(data.table)
library(purrr)
library(stringr)
library(glue)
library(readr)
library(fasttime)

# functions for this stage alone
ci <- function(x){qnorm(0.975)*sd(x, na.rm = T)/sqrt((length(x)))}
```

## Patch construction

```{r remove_old_data, eval=FALSE}
if(file.exists("data/data2018/data_2018_tidal_mean_speed.csv")){
  file.remove("data/data2018/data_2018_tidal_mean_speed.csv")
}

if(file.exists("data/data2018/data_2018_patch_summary.csv")){
  file.remove("data/data2018/data_2018_patch_summary.csv")
}
```

Process patches. Takes approx. 5 hours for 3 second data.

```{r make_patches, eval=FALSE, message=FALSE, warning=FALSE}
# make a vector of data files to read
data_files <- list.files(path = "data/data2018/revisitData", 
                         pattern = "_revisit.csv", full.names = TRUE)

# get tag ids
data_id <- str_split(data_files, "/") %>% 
  map(function(l)l[[4]] %>% str_sub(1,3)) %>% 
  flatten_chr()

# make df of tag ids and files
data <- data_frame(tag = data_id, data_file = data_files)
data <- split(x = data, f = data$tag) %>% 
  map(function(l) l$data_file)

# map inferResidence, classifyPath, and getPatches over data
for(i in 1:length(data)){
  patch_data <- map(data[[i]], function(l){
  
    # read the data file
    temp_data <- fread(l)
    temp_data[,ts:=fastPOSIXct(ts)]
    
    id <- unique(temp_data$id)
    tide_number <- unique(temp_data$tide_number)
    
    # get data summary for the tidal cycle
    {
      data_summary <- temp_data[,.(duration = (max(time) - min(time))/60,
                                   n_fixes = length(x),
                                   prop_fixes = length(x) / ((max(time) - min(time))/30)),
                                by = .(id, tide_number)]
      
      sld <- watlastools::wat_simple_dist(temp_data, "x", "y")
      timelag <- c(NA, as.numeric(diff(temp_data$time)))
      speed <- sld/timelag
      
      data_summary[,`:=`(mean_speed = mean(speed, na.rm=TRUE),
                         ci95_speed = ci(speed))]
      
      # write total distance for the tidal cycle
      fwrite(data_summary, file = "data/data2018/data_2018_tidal_mean_speed.csv", 
             append = TRUE, scipen = 6)
    }
    
    # wrap process in try catch
    tryCatch(
      {
        # watlastools function to infer residence
        temp_data <- wat_infer_residence(df = temp_data,
                                         infPatchTimeDiff = 30,
                                         infPatchSpatDiff = 100)
        
        # watlastools function to classify path
        temp_data <- wat_classify_points(somedata = temp_data,
                                         resTimeLimit = 2)
        
        # watlastools function to get patches
        patch_dt <- wat_make_res_patch(somedata = temp_data,
                                         bufferSize = 10,
                                         spatIndepLim = 100,
                                         tempIndepLim = 30,
                                         restIndepLim = 30,
                                         minFixes = 3)
        # print message
        message(as.character(glue('patches {id}_{tide_number} done')))
        return(patch_dt)
      },
      # null error function, with option to collect data on errors
      error= function(e)
      {
        message(glue::glue('patches {id}_{tide_number} errored'))
      }
    )
  })
  
  # repair high tide patches across an individual's tidal cycles
  repaired_data <- wat_repair_ht_patches(patch_data_list = patch_data)
  # write patch summary data
  
  if(all(is.data.frame(repaired_data), nrow(repaired_data) > 0)){
    # watlastools function to get patch data as spatial
    patch_summary <- wat_get_patch_summary(resPatchData = repaired_data,
                                           whichData = "summary")
    fwrite(patch_summary, file = "data/data2018/data_2018_patch_summary.csv",
           append = TRUE)
  }
}

```

<!--chapter:end:05_residencePatches.rmd-->

---
editor_options: 
  chunk_output_type: console
---

# Finding spatial 

## Load libs

```{r prep_libs, eval=FALSE}
# for data
library(tidyverse)
# python options
library(reticulate)
# set python path
use_python("/usr/bin/python3")
```

## Prepare data

Load data and filter for quality. Keep tides with at least 10 individuals, and inviduals recorded in at least 10 tides.

```{r get_data, eval=FALSE}
# get data and filter for individuals represented in 10 tides
# and tides with 10 or more individuals
data <- read_csv("data/data_2018_patch_summary.csv")

good_tides <- group_by(data, tide_number) %>% 
  summarise(birds_in_tide = length(unique(id))) %>% 
  filter(birds_in_tide >= 10)
good_birds <- group_by(data, id) %>% 
  summarise(tides_present = length(unique(tide_number))) %>% 
  filter(tides_present >= 10)

# now filter the data
data <- filter(data, 
               tide_number %in% good_tides$tide_number,
               id %in% good_birds$id)

# write good patches
write_csv(data, "data/data_2018_good_patches.csv")
```

## Prepare Python libraries.

```{python prep_py_libs_supp07, eval=FALSE, message=FALSE, warning=FALSE}

# network lib
import networkx as nx
import numpy as np
import pandas as pd

# import ckdtree
from scipy.spatial import cKDTree


def round_any(value, limit):
    return round(value/limit)*limit


# function to use ckdtrees for nearest point finding
def make_patch_pairs(patch_data, dist_indep):
    coords = patch_data[['x_mean', 'y_mean']]
    coords = np.asarray(coords)
    ckd_tree = cKDTree(coords)
    pairs = ckd_tree.query_pairs(r=dist_indep, output_type='ndarray')
    return pairs


# make modules from patch data
# function to process ckd_pairs
def make_patch_modules(patch_data, scale):
    # assign a unique id per dataframe
    patch_data['within_tide_id'] = np.arange(len(patch_data))
    patch_pairs = make_patch_pairs(patch_data=patch_data, dist_indep=scale)
    if len(patch_pairs) > 1:
        patch_pairs = pd.DataFrame(data=patch_pairs, columns=['p1', 'p2'])
        # get unique patch ids
        unique_patches = np.concatenate((patch_pairs.p1.unique(), patch_pairs.p2.unique()))
        unique_patches = np.unique(unique_patches)
        # make network
        network = nx.from_pandas_edgelist(patch_pairs, 'p1', 'p2')
        # get modules
        modules = list(nx.algorithms.community.greedy_modularity_communities(network))
        # get modules as df
        m = []
        for i in np.arange(len(modules)):
            module_number = [i] * len(modules[i])
            module_coords = list(modules[i])
            m = m + list(zip(module_number, module_coords))
        # add location, bird and tide
        aux_data = patch_data[patch_data.within_tide_id.isin(unique_patches)][
            ['id', 'tide_number', 'patch', 'within_tide_id', 'time_scale',
             'time_chunk']]
        module_data = pd.DataFrame(m, columns=['module', 'within_tide_id'])
        module_data = pd.merge(module_data, aux_data, on='within_tide_id')
        # add scale
        module_data['spatial_scale'] = scale
        return module_data
    else:
        return None

```

Send the list of data frames to Python.

```{r send_to_py, eval=FALSE}
# send to python
py$data_py = data
```

## Find patch clusters

```{python, find_patch_clusters, eval=FALSE}
# run code

import os
import pandas as pd
import numpy as np
import itertools
from helper_functions import make_patch_modules, round_any

# read in the data
data = pd.read_csv("data/data_2018_good_patches.csv")  # use good_patches for quality control
data.head()
# look at one tidal cycle, first count patches per tide
pd.value_counts(data['tide_number'])
# choose the highest
# data = data[data['tide_number'] == 73]

# assign rounded values of time rather than tide number
# do this in a list
time_scale = [1, 3, 6, 12]

data_list = []
# what is the min of time
min_time = data.time_mean.min()/3600
for i in np.arange(len(time_scale)):
    tmp_data = data
    tmp_data['round_time'] = round_any((tmp_data['time_mean']/3600) - min_time,
                                       time_scale[i])
    tmp_data['time_scale'] = time_scale[i]
    data_list.append([pd.DataFrame(y) for x, y in
                      tmp_data.groupby('round_time',
                                       as_index=False)])

# add time chunk
for i in np.arange(len(data_list)):
    for j in np.arange(len(data_list[i])):
        data_list[i][j]['time_chunk'] = j

# flatten this list, time_scale is stored in each df
data_list = list(itertools.chain(*data_list))

# remove lists with single patch
data_list = [df for df in data_list if len(df) > 1]

# now get modules over spatial scales
# there are 4 list elements of temporal scale
# times 4 spatial scales
# run over spatial scales 100, 250, 500, 1000
spatial_scales = [50, 100, 250, 500]
ml_list = list(map(lambda x:
                   list(map(make_patch_modules, data_list, [x]*len(data_list))),
                   spatial_scales))


# flatten module list
ml_list = list(itertools.chain(*ml_list))
ml_list2 = [i for i in ml_list if i is not None]

# concatenate data
ml_data = pd.concat(ml_list2)

# write to file
ml_data.to_csv(index=False, path_or_buf="data/data_2018_patch_modules_small_scale.csv")

```

<!--chapter:end:06_finding_spatial_modules.rmd-->

---
editor_options: 
  chunk_output_type: console
---

# Knots in modules

## Prepare libraries

```{r prep_libs_02, eval=FALSE}
library(tidyverse)
# for plots
library(ggplot2)
library(scico)

# ci function
ci <- function(x){qnorm(0.975)*sd(x, na.rm = TRUE)/sqrt(length(x))}
```

## Read in data

```{r read_modules, eval=FALSE}
# read in data
modules <- read_csv("data/data_2018_patch_modules_small_scale.csv")
```

## Summarise patches per module

```{r summarise_data, eval=FALSE}
# count modules at each scale and plot
module_per_scale <- modules %>% 
  group_by(time_scale, spatial_scale, time_chunk) %>% 
  summarise_at(vars(module),
               list(~length(.))) %>% 
  group_by(time_scale, spatial_scale) %>% 
  summarise(mean_n_modules = mean(module))

fig_modules_scale <-
ggplot(module_per_scale)+
  geom_tile(aes(factor(time_scale), factor(spatial_scale), 
                fill = mean_n_modules), col = "grey")+
  scale_fill_scico(palette = "nuuk",
                   # limits = c(NA, 1000),
                   direction = -1,
                   trans = "log10")+
  theme_test()+
  theme(legend.key.width = unit(0.1, "cm"))+
  labs(x = "time scale (hours)", y = "spatial scale (m)",
       fill = "spatial\nmodules", title = "(a)")
```

## Summarise modules per id per tide

How many ids in each scale, on average?

```{r summarise_by_id, eval=FALSE}
# count unique individuals per time chunk per module per scale
id_per_scale <- modules %>% 
  group_by(time_scale, spatial_scale, time_chunk, module) %>% 
  summarise(unique_id = length(unique(id))) %>% 
  group_by(time_scale, spatial_scale) %>% 
  summarise(mean_uid = mean(unique_id))

fig_id_scale <- 
ggplot(id_per_scale)+
  geom_tile(aes(factor(time_scale), factor(spatial_scale), 
                fill = mean_uid), col = "grey")+
  scale_fill_scico(palette = "broc",
                   limits = c(2, NA),
                   na.value = "grey",
                   direction = 1)+
  theme_test()+
  theme(legend.key.width = unit(0.1, "cm"))+
  labs(x = "time scale (hours)", y = "spatial scale (m)",
       fill = "birds", title = "(b)")
```

```{r save_fig_scale, eval=FALSE}
# make patchwork and export
fig_scales <- patchwork::wrap_plots(fig_modules_scale, fig_id_scale)
ggsave(fig_scales, filename = "figs/fig_modules_scale_small.png", 
       dpi = 300, height = 3, width = 7)
```

## Does number of patches scale with individuals in module?

```{r patch_size_module_size, eval=FALSE}
# get module size at each scale
patch_per_mod <- modules %>% 
  group_by(time_scale, spatial_scale, time_chunk, module) %>% 
  summarise(n_patches = length(patch),
            n_uid = length(unique(id))) 

fig_patches_id <- ggplot(patch_per_mod)+
  geom_abline(slope = c(1,2,3), 
              col = c("red"),
              size = 0.1)+
  geom_point(aes(n_uid, n_patches), 
             size = 0.1, alpha = 0.2,
             col = "steelblue")+
  theme_test()+
  facet_grid(time_scale~spatial_scale,
             labeller = label_both,
             scales = "free_y",
             as.table = FALSE,
             switch = "both")+
  labs(x = "unique individuals",
       y = "# patches")

# save figure
ggsave(fig_patches_id, filename = "figs/fig_patches_id.png",
       dpi = 300)
```

```{r}
knitr::include_graphics("figs/fig_patches_id.png")
```


<!--chapter:end:07_module_descriptions.rmd-->

---
editor_options: 
  chunk_output_type: console
---

# Bathymetry of the Griend mudflats

## Get data and plot basic trend

```{r load_libs_s01, eval=FALSE}
# load libs to read bathymetry
library(raster)
library(rayshader)
library(sf)

# data libs
library(data.table)
library(purrr)
library(stringr)

# plot libs
library(ggplot2)
library(ggthemes)
```

```{r load_data_s01, eval=FALSE}
# load bathymetry and subset
data <- raster("data/bathymetry_waddenSea_2015.tif")
griend <- st_read("griend_polygon/griend_polygon.shp") %>% 
  st_buffer(10.5e3)

# assign utm 31 crs and subset
crs(data) <- unlist(st_crs(griend)[2])
data <- crop(data, as(griend, "Spatial"))
data <- aggregate(data, fact = 2)
# remove 20m outliers
#data[data > 500] <- NA
data_m <- raster_to_matrix(data)

# get quantiles of the matrix
data_q <- quantile(data_m, na.rm = T, probs = 1:1000/1000)
```

```{r load_waterlevel_s01, eval=FALSE}
# read in waterlevel data
waterlevel <- fread("data/data2018/waterlevelWestTerschelling.csv", sep = ";")

# select useful columns and rename
waterlevel <- waterlevel[,.(WAARNEMINGDATUM, WAARNEMINGTIJD, NUMERIEKEWAARDE)]

setnames(waterlevel, c("date", "time", "level"))

# make a single POSIXct column of datetime
waterlevel[,dateTime := as.POSIXct(paste(date, time, sep = " "), 
                                   format = "%d-%m-%Y %H:%M:%S", tz = "CET")]

waterlevel <- setDT(distinct(setDF(waterlevel), dateTime, .keep_all = TRUE))
```

```{r plot_bathymetry_quantiles, eval=FALSE}
# plot waterlevel quantiles with data from west terschelling
waterlimits <- range(waterlevel$level)

fig_waterlevel_area <- ggplot()+
  geom_line(aes(x = 1:1000/1000, y = data_q))+
  geom_hline(yintercept = waterlimits, col = "blue", lty = 2)+
  geom_hline(yintercept = 0, lty = 2, col = "red")+
  scale_x_continuous(labels = scales::percent)+
  theme_bw()+
  coord_flip(xlim = c(0.25, 1.01), ylim = c(-250, 250))+
  labs(x = "% area covered", y = "waterlevel (cm over NAP)")

ggsave(fig_waterlevel_area, filename = "figs/fig_waterlevel_area.png",
       dpi = 300, height = 4, width = 6); dev.off()
```

## Plot as 3D maps

```{r make_water_seq, eval=FALSE}
# make sequence
waterdepth <- seq(waterlimits[1], waterlimits[2], length.out = 30)
waterdepth <- c(waterdepth, rev(waterdepth))
```


```{r vis_data, eval=FALSE}
# make visualisation
for(i in 1:length(waterdepth)) {
  data_m %>%
    sphere_shade(texture = "imhof1", zscale = 50) %>%
    # add_water(detect_water(data_m, zscale = 100, 
    #                        max_height = waterdepth[i],cutoff = 0.1), 
    #           color = "desert") %>%
    add_shadow(hillshade = data_m) %>%
    plot_3d(data_m, zscale = 75, wateralpha = 0.6,
            solid =F, shadow=F,
            water = TRUE,
            waterdepth = waterdepth[i],
            watercolor = "dodgerblue1",
            phi = 90,
            theta = 0, zoom = 0.75, windowsize = c(1000, 800),
            background = "black", calculate_normals = F)
  render_label(data_m, x = 500, y = 500, z = 500,
               text = glue::glue('waterdepth = {round(waterdepth[i])} cm'), 
               freetype = F, textcolor = "black")
    
  rgl::snapshot3d(paste0("figs/tide_rise/fig",str_pad(i, 2, pad = "0"),".png"))
  rgl::rgl.close()
}
```

```{r make_tide_gif, eval=FALSE}
library(magick)
list.files(path = "figs/tide_rise/", pattern = "*.png", full.names = T) %>% 
  map(image_read) %>% # reads each path file
  image_join() %>% # joins image
  image_animate(fps=2) %>% # animates, can opt for number of loops
  image_write("figs/fig_tide_rise_anim.gif") # write to current dir
```


```{r include_gif, eval=TRUE, fig.cap="Waterlevel at West Terschelling and effect on coverage of mudflats around Griend."}
knitr::include_graphics("figs/fig_tide_rise_anim.gif")
```


<!--chapter:end:08_supplement01_waterlevelGriend.rmd-->

