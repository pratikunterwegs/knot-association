---
editor_options: 
  chunk_output_type: console
---

# Finding spatio-temporal patch clusters

## Load libs

```{r prep_libs_06, eval=FALSE}
# for data
library(tidyverse)
# python options
library(reticulate)
# set python path
use_python("/usr/bin/python3")
```

## Prepare patch data

Load data and filter for quality. Keep tides with at least 10 individuals, and inviduals recorded in at least 10 tides.

```{r get_data06, eval=FALSE}
# get data and filter for individuals represented in 10 tides
# and tides with 10 or more individuals
data <- read_csv("data/data_2018_patch_summary.csv")

good_tides <- group_by(data, tide_number) %>% 
  summarise(birds_in_tide = length(unique(id))) %>% 
  filter(birds_in_tide >= 10)
good_birds <- group_by(data, id) %>% 
  summarise(tides_present = length(unique(tide_number))) %>% 
  filter(tides_present >= 10)

# now filter the data
data <- filter(data, 
               tide_number %in% good_tides$tide_number,
               id %in% good_birds$id)

# write good patches
write_csv(data, "data/data_2018_good_patches.csv")
```

## Prepare Python libraries

```{python prep_py_libs_supp07, eval=FALSE, message=FALSE, warning=FALSE}

# network lib
import networkx as nx
import numpy as np
import pandas as pd

# import ckdtree
from scipy.spatial import cKDTree


def round_any(value, limit):
    return round(value/limit)*limit


# function to use ckdtrees for nearest point finding
def make_patch_pairs(patch_data, dist_indep):
    coords = patch_data[['x_mean', 'y_mean']]
    coords = np.asarray(coords)
    ckd_tree = cKDTree(coords)
    pairs = ckd_tree.query_pairs(r=dist_indep, output_type='ndarray')
    return pairs


# make modules from patch data
# function to process ckd_pairs
def make_patch_modules(patch_data, scale):
    # assign a unique id per dataframe
    patch_data['within_tide_id'] = np.arange(len(patch_data))
    patch_pairs = make_patch_pairs(patch_data=patch_data, dist_indep=scale)
    if len(patch_pairs) > 1:
        patch_pairs = pd.DataFrame(data=patch_pairs, columns=['p1', 'p2'])
        # get unique patch ids
        unique_patches = np.concatenate((patch_pairs.p1.unique(), patch_pairs.p2.unique()))
        unique_patches = np.unique(unique_patches)
        # make network
        network = nx.from_pandas_edgelist(patch_pairs, 'p1', 'p2')
        # get modules
        modules = list(nx.algorithms.community.greedy_modularity_communities(network))
        # get modules as df
        m = []
        for i in np.arange(len(modules)):
            module_number = [i] * len(modules[i])
            module_coords = list(modules[i])
            m = m + list(zip(module_number, module_coords))
        # add location, bird and tide
        aux_data = patch_data[patch_data.within_tide_id.isin(unique_patches)][
            ['id', 'tide_number', 'patch', 'within_tide_id', 'time_scale',
             'time_chunk']]
        module_data = pd.DataFrame(m, columns=['module', 'within_tide_id'])
        module_data = pd.merge(module_data, aux_data, on='within_tide_id')
        # add scale
        module_data['spatial_scale'] = scale
        return module_data
    else:
        return None

```

Send the list of data frames to Python.

```{r send_to_py, eval=FALSE}
# send to python
py$data_py = data
```

## Find patch clusters

```{python, find_patch_clusters, eval=FALSE}
# run code

import os
import pandas as pd
import numpy as np
import itertools
from helper_functions import make_patch_modules, round_any

# read in the data
data = pd.read_csv("data/data_2018_good_patches.csv")  # use good_patches for quality control
data.head()
# look at one tidal cycle, first count patches per tide
pd.value_counts(data['tide_number'])
# choose the highest
# data = data[data['tide_number'] == 73]

# assign rounded values of time rather than tide number
# do this in a list
time_scale = [1, 3, 6, 12]

data_list = []
# what is the min of time
min_time = data.time_mean.min()/3600
for i in np.arange(len(time_scale)):
    tmp_data = data
    tmp_data['round_time'] = round_any((tmp_data['time_mean']/3600) - min_time,
                                       time_scale[i])
    tmp_data['time_scale'] = time_scale[i]
    data_list.append([pd.DataFrame(y) for x, y in
                      tmp_data.groupby('round_time',
                                       as_index=False)])

# add time chunk
for i in np.arange(len(data_list)):
    for j in np.arange(len(data_list[i])):
        data_list[i][j]['time_chunk'] = j

# flatten this list, time_scale is stored in each df
data_list = list(itertools.chain(*data_list))

# remove lists with single patch
data_list = [df for df in data_list if len(df) > 1]

# now get modules over spatial scales
# there are 4 list elements of temporal scale
# times 4 spatial scales
# run over spatial scales 100, 250, 500, 1000
spatial_scales = [50, 100, 250, 500]
ml_list = list(map(lambda x:
                   list(map(make_patch_modules, data_list, [x]*len(data_list))),
                   spatial_scales))


# flatten module list
ml_list = list(itertools.chain(*ml_list))
ml_list2 = [i for i in ml_list if i is not None]

# concatenate data
ml_data = pd.concat(ml_list2)

# write to file
ml_data.to_csv(index=False, path_or_buf="data/data_2018_patch_modules_small_scale.csv")

```
