---
editor_options: 
  chunk_output_type: console
---

# Applying CTMM to WATLAS data

CTMM [@some citation] can estimate speed from poor quality data. However, CTMM has its limits, and fails most frequently _when the movement model is identified as being fractal_. A solution to this is to decrease the temporal resolution of the data _within_ residence patches, i.e., to aggregate positions over a certain interval. However, data resulting from this procedure may run afoul of another CTMM feature, _outlier removal_, which may recommend the removal of nearly all points in a trajectory. This depends on the percentile of the data (in terms of speed and distance from the centroid(?)) considered to be outliers, as well as on the scale of aggregation used earlier to avoid fractal movement models.

Here, we examine the effect of different aggregation scales on the proportion of patches that would be discarded.

## Load libraries

```{r load_libs_supp_01, eval=FALSE}
# load libs
library(data.table)
library(lubridate)
library(sf)
library(glue)
library(stringr)
library(fasttime)
library(tibble)
library(dplyr)
library(purrr)
library(tidyr)

# devtools::install_github("pratikunterwegs/watlasUtils", ref = "devbranch")
library(watlasUtils)
library(ctmm)
```

## Load a random subset of data

```{r prep_data_load_s01, eval=FALSE}
# make a list of data files to read
data_files <- list.files(path = "data/data2018/revisitData", pattern = "_revisit.csv", full.names = TRUE)

# select a random subset of around 1%
data_files <- data_files[runif(length(data_files)) < 0.01]
```

## Choose scales of aggregation

```{r choose_agg_scale, eval=FALSE}
scales <- c(15, 30, 60)

data_to_test <- crossing(scales, data_files)
```

```{r count_patches_remaining}
rm(data_files, scales)
some_output <- pmap(data_to_test, function(scales, data_files){
  {
    temp_data <- fread(data_files)
    temp_data[,ts:=fastPOSIXct(ts)]
    
    id <- unique(temp_data$id)
    tide_number <- unique(temp_data$tide_number)
    
    success = FALSE
    #ratio = NA
    # wrap process in try catch
    tryCatch(
      {
        # watlasUtils function to infer residence
        temp_data <- wat_infer_residence(df = temp_data,
                                         infResTime = 2,
                                         infPatchTimeDiff = 30,
                                         infPatchSpatDiff = 100)
        
        # watlasUtils function to classify path
        temp_data <- wat_classify_points(somedata = temp_data,
                                         resTimeLimit = 2)
        
        # watlasUtils function to get patches
        patch_data <- wat_make_res_patch(somedata = temp_data,
                                         bufferSize = 10,
                                         spatIndepLim = 100,
                                         tempIndepLim = 30,
                                         restIndepLim = 30,
                                         minFixes = 3,
                                         tideLims = c(4,10))
        
        # get patch data points
        patch_points <- wat_get_patch_summary(resPatchData = patch_data,
                                              dataColumn = "data",
                                              whichData = "points")
        success = TRUE
      },
      # null error function, with option to collect data on errors
      error= function(e)
      {
        message(glue::glue('patches {id}_{tide_number} errored'))
      }
      
    )
  }
  
  if(success == TRUE){
    tryCatch({
      # prepare for telemetry
      {
        data_for_ctmm <- setDT(patch_points)[,.(id, tide_number, x, y, patch, time, VARX, VARY)]
        
        # count points in patch and filter
        data_for_ctmm <- data[, ]
        
        # aggregate within a patch to 10 seconds
        data_for_ctmm <- split(data_for_ctmm, f = data_for_ctmm$patch) %>% 
          map(wat_agg_data, interval = scales) %>% 
          bind_rows()
        
        # make each patch an indiv
        setDT(data_for_ctmm)
        data_for_ctmm[,individual.local.identifier:= paste(id, tide_number, patch,
                                                           sep = "_")]
        # get horizontal error
        data_for_ctmm[,HDOP := sqrt(VARX+VARY)/10]
        # subset columns
        data_for_ctmm <- data_for_ctmm[,.(individual.local.identifier, time, x, y, HDOP)]
        
        # get new names
        setnames(data_for_ctmm, old = c("x", "y", "time"), 
                 new = c("UTM.x","UTM.y", "timestamp"))
        
        # convert time to posixct
        data_for_ctmm[,timestamp:=as.POSIXct(timestamp, origin = "1970-01-01")]
        # add UTM zone
        data_for_ctmm[,zone:="31 +north"]
        
      }
      
      # make telemetry
      {
        tel <- as.telemetry(data_for_ctmm)
      }
      
      n_patches <- length(tel)
      
      # ctmm section
      {
        # get the outliers but do not plot
        outliers <- map(tel, outlie, plot=FALSE)
        # get a list of 99 th percentile outliers
        q90 <- map(outliers, function(this_outlier_set){
          quantile(this_outlier_set[[1]], probs = c(0.99))
        })
        # remove outliers from telemetry data
        tel <- pmap(list(tel, outliers, q90), 
                    function(this_tel_obj, this_outlier_set, outlier_quantile) 
                    {this_tel_obj[-(which(this_outlier_set[[1]] >= outlier_quantile)),]})
        
        # some patches have no data remaining, filter them out
        tel <- keep(tel, function(this_tel){nrow(this_tel) > 0})
        
        r_patches <- length(tel)
      }
      ratio = r_patches/n_patches
      message(ratio)
      return(ratio)
    },
    error= function(e)
    {
      return(NA)
    })
  }else{
    return(NA)
  }
  
})

```

