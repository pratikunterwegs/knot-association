[
["index.html", "Individual consistency in space-use across scales 1 Introduction 1.1 Attribution 1.2 Data access 1.3 Data processing", " Individual consistency in space-use across scales Pratik R Gupte 2020-02-11 1 Introduction This is the bookdown version of a project in preparation that links high-resolution tracking data from individual red knots Calidris canutus islandica to fine-scale experimental behaviour measurements in captivity, and examines whether individuals are consistent across scales in their space-use. 1.1 Attribution Please contact the following before cloning or in case of interest in the project. Pratik Gupte (author and maintainer) PhD student, GELIFES – University of Groningen Guest researcher, COS – NIOZ p.r.gupte@rug.nl Nijenborgh 7/5172.0583 9747AG Groningen Allert Bijleveld (PI): allert.bijleveld@nioz.nl Project information: https://www.nioz.nl/en/about/cos/coastal-movement-ecology/shorebird-tracking Selin Ersoy (collab): selin.ersoy@nioz.nl 1.2 Data access The data used in this work are not publicly available. Contact PI Allert Bijleveld for data access. 1.3 Data processing The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar. "],
["getting-data.html", "2 Getting data 2.1 Prepare watlasUtils and other libraries 2.2 Read in tag deployment data 2.3 Get data and save locally", " 2 Getting data This section focusses on accessing and downloading WATLAS data. This is done using functions in the WATLAS Utilities package. Workflow Preparing required libraries. Reading tag data with deployment start dates from a local file. This file is not yet publicly available. Connecting to the NIOZ databse and downloading data. This database is also not public-access. 2.1 Prepare watlasUtils and other libraries # install the package watlasUtils from master branch using the following # install.packages(&quot;devtools&quot;) library(devtools) # devtools::install_github(&quot;pratikunterwegs/watlasUtils&quot;) library(watlasUtils) # libraries to process data library(data.table) library(ggplot2) library(ggthemes) library(purrr) library(glue) 2.2 Read in tag deployment data # read deployment data from local file in data folder tag_info &lt;- fread(&quot;data/data2018/SelinDB.csv&quot;) # filter out NAs in release date and time tag_info &lt;- tag_info[!is.na(Release_Date) &amp; !is.na(Release_Time),] # make release date column as POSIXct tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = &quot; &quot;), format = &quot;%d.%m.%y %H:%M&quot;, tz = &quot;CET&quot;)] # check new release date column head(tag_info$Release_Date) ## [1] &quot;2018-09-14 20:00:00 CEST&quot; ## [2] &quot;2018-09-13 15:30:00 CEST&quot; ## [3] &quot;2018-09-14 20:00:00 CEST&quot; ## [4] &quot;2018-09-13 18:30:00 CEST&quot; ## [5] &quot;2018-08-11 15:00:00 CEST&quot; ## [6] &quot;2018-08-16 18:04:00 CEST&quot; (#fig:plot_release_schedule)Knots released per week of 2018. 2.3 Get data and save locally # read in database access parameters from a local file data_access &lt;- fread(&quot;data/access_params.txt&quot;) # create a data storage file if not present # use the getData function from watlasUtils on the tag_info data frame # this is placed inside a pmap wrapper to automate access for all birds if(!dir.exists(&quot;data/data2018&quot;)) { dir.create(&quot;data/data2018&quot;) } pmap(tag_info[,.(Toa_Tag, Release_Date)], function(Toa_Tag, Release_Date){ prelim_data &lt;- watlasUtils::wat_get_data(tag = Toa_Tag, tracking_time_start = as.character(Release_Date), tracking_time_end = &quot;2018-10-31&quot;, username = data_access$username, password = data_access$password) setDT(prelim_data) # prelim_data[,TAG:= = as.numeric(TAG) - prefix_num] message(glue(&#39;tag {Toa_Tag} accessed with {nrow(prelim_data)} fixes&#39;)) fwrite(prelim_data, file = glue(&#39;data/data2018/{Toa_Tag}_data.csv&#39;), dateTimeAs = &quot;ISO&quot;) }) "],
["cleaning-data.html", "3 Cleaning data 3.1 Prepare watlasUtils and other libraries 3.2 Prepare to remove attractor points 3.3 Read, clean, and write data", " 3 Cleaning data This section is about cleaning downloaded data using the cleanData function in the WATLAS Utilities package. Workflow Prepare required libraries. Read in data, apply the cleaning function, and overwrite local data. 3.1 Prepare watlasUtils and other libraries # watlasUtils assumed installed from the previous step # if not, install from the github repo as shown below devtools::install_github(&quot;pratikunterwegs/watlasUtils&quot;) library(watlasUtils) # libraries to process data library(data.table) library(purrr) library(glue) library(fasttime) library(bit64) library(stringr) 3.2 Prepare to remove attractor points # read in identified attractor points atp &lt;- fread(&quot;data/data2018/attractor_points.txt&quot;) 3.3 Read, clean, and write data # make a list of data files to read data_files &lt;- list.files(path = &quot;data/data2018/&quot;, pattern = &quot;whole_season*&quot;, full.names = TRUE) data_ids &lt;- str_extract(data_files, &quot;(tx_\\\\d+)&quot;) %&gt;% str_sub(-3,-1) # read deployment data from local file in data folder tag_info &lt;- fread(&quot;data/data2018/SelinDB.csv&quot;) # filter out NAs in release date and time tag_info &lt;- tag_info[!is.na(Release_Date) &amp; !is.na(Release_Time),] # make release date column as POSIXct tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = &quot; &quot;), format = &quot;%d.%m.%y %H:%M&quot;, tz = &quot;CET&quot;)] # sub for knots in data data_files &lt;- data_files[as.integer(data_ids) %in% tag_info$Toa_Tag] # map read in, cleaning, and write out function over vector of filenames map(data_files, function(df){ temp_data &lt;- fread(df, integer64 = &quot;numeric&quot;) # filter for release date { temp_id &lt;- str_sub(temp_data[1, TAG], -3, -1) rel_date &lt;- tag_info[Toa_Tag == temp_id, Release_Date] temp_data &lt;- temp_data[TIME/1e3 &gt; (rel_date),] } temp_data &lt;- wat_rm_attractor(df = temp_data, atp_xmin = atp$xmin, atp_xmax = atp$xmax, atp_ymin = atp$ymin, atp_ymax = atp$ymax) clean_data &lt;- wat_clean_data(somedata = temp_data, moving_window = 5, nbs_min = 0, sd_threshold = 5e5) message(glue(&#39;tag {unique(clean_data$id)} cleaned with {nrow(clean_data)} fixes&#39;)) fwrite(x = clean_data, file = df, dateTimeAs = &quot;ISO&quot;) rm(temp_data, clean_data) }) "],
["adding-tidal-cycle-data.html", "4 Adding tidal cycle data 4.1 Prepare libraries 4.2 Read water level data 4.3 Calculate high tides 4.4 Add time since high tide", " 4 Adding tidal cycle data This section is about adding tidal cycle data to individual trajectories. This is done to split the data up into convenient, and biologically sensible units. This section uses the package VulnToolkit (Troy D. Hill, Shimon C. Anisfeld 2014) to identify high tide times from water-level data provided by Rijkswaterstaat for the measuring point at West Terschelling. Workflow Prepare required libraries, Read in water level data and identify high tides, Write tidal cycle data to local file, Add time since high tide to movement data. 4.1 Prepare libraries # load VulnToolkit or install if not available if(&quot;VulnToolkit&quot; %in% installed.packages() == FALSE){ devtools::install_github(&quot;troyhill/VulnToolkit&quot;) } library(VulnToolkit) # libraries to process data library(data.table) library(purrr) library(glue) library(dplyr) library(stringr) library(fasttime) 4.2 Read water level data Water level data for West Terschelling, a settlement approx. 10km from the field site are provided by Rijkswaterstaat’s Waterinfo, in cm above Amsterdam Ordnance Datum. These data are manually downloaded in the range July 1, 2018 – October 31, 2018 and saved in data/data2018. # read in waterlevel data waterlevel &lt;- fread(&quot;data/data2018/waterlevelWestTerschelling.csv&quot;, sep = &quot;;&quot;) # select useful columns and rename waterlevel &lt;- waterlevel[,.(WAARNEMINGDATUM, WAARNEMINGTIJD, NUMERIEKEWAARDE)] setnames(waterlevel, c(&quot;date&quot;, &quot;time&quot;, &quot;level&quot;)) # make a single POSIXct column of datetime waterlevel[,dateTime := as.POSIXct(paste(date, time, sep = &quot; &quot;), format = &quot;%d-%m-%Y %H:%M:%S&quot;, tz = &quot;CET&quot;)] waterlevel &lt;- setDT(distinct(setDF(waterlevel), dateTime, .keep_all = TRUE)) 4.3 Calculate high tides A tidal period of 12 hours 25 minutes is taken from Rijkswaterstaat. # use the HL function from vulnToolkit to get high tides tides &lt;- VulnToolkit::HL(waterlevel$level, waterlevel$dateTime, period = 12.41, tides = &quot;H&quot;, semidiurnal = TRUE) # read in release data and get first release - 24 hrs tag_info &lt;- fread(&quot;data/data2018/SelinDB.csv&quot;) tag_info &lt;- tag_info[!is.na(Release_Date) &amp; !is.na(Release_Time),] tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = &quot; &quot;), format = &quot;%d.%m.%y %H:%M&quot;, tz = &quot;CET&quot;)] first_release &lt;- min(tag_info$Release_Date) - (3600*24) # remove tides before first release tides &lt;- setDT(tides)[time &gt; first_release, ][,tide2:=NULL] tides[,tide_number:=1:nrow(tides)] # write to local file fwrite(tides, file = &quot;data/data2018/tidesSummer2018.csv&quot;, dateTimeAs = &quot;ISO&quot;) 4.4 Add time since high tide # read in data and add time since high tide data_files &lt;- list.files(path = &quot;data/data2018/&quot;, pattern = &quot;whole_season*&quot;, full.names = TRUE) data_ids &lt;- str_extract(data_files, &quot;(tx_\\\\d+)&quot;) %&gt;% str_sub(-3,-1) data_files &lt;- data_files[as.integer(data_ids) %in% tag_info$Toa_Tag] # map read in and tidal time calculation over data # merge data to insert high tides within movement data # arrange by time to position high tides correctly map(data_files, function(df){ # read and fix data types temp_data &lt;- fread(df, integer64 = &quot;numeric&quot;) temp_data[,ts:=fastPOSIXct(ts, tz = &quot;CET&quot;)] # merge with tides and order on time temp_data &lt;- merge(temp_data, tides, by.x = &quot;ts&quot;, by.y = &quot;time&quot;, all = TRUE) setorder(temp_data, ts) # get tide_number and time since high tide temp_data[, tide_number := nafill(tide_number, &quot;locf&quot;)] temp_data[, tidaltime := as.numeric(difftime(ts, ts[1], units = &quot;mins&quot;)), by = tide_number] # remove bad rows and columns temp_data &lt;- temp_data[complete.cases(x),] temp_data[,`:=`(tide = NULL, level = NULL)] # export data, print msg, remove data fwrite(temp_data, file = df, dateTimeAs = &quot;ISO&quot;) message(glue(&#39;tag {unique(temp_data$id)} added time since high tide&#39;)) rm(temp_data) }) "],
["revisit-analysis.html", "5 Revisit analysis 5.1 Prepare libraries 5.2 Read data, split, recurse, write", " 5 Revisit analysis This section is about splitting the data by tidal cycle, and passing the individual- and tidal cycle-specific data to revisit analysis, which is implemented using the package recurse. Workflow Prepare required libraries, Performing recurse: Read in movement data and split by tidal cycle, Perform revisit analysis using recurse, Write data with revisit metrics to file. 5.1 Prepare libraries This section uses the recurse package (Bracis, Bildstein, and Mueller 2018). # load recurse or install if not available if(&quot;recurse&quot; %in% installed.packages() == FALSE){ install.packages(&quot;recurse&quot;) } library(recurse) # libraries to process data library(data.table) library(purrr) library(glue) library(dplyr) library(fasttime) 5.2 Read data, split, recurse, write # read in data data_files &lt;- list.files(path = &quot;data/data2018/&quot;, pattern = &quot;whole_season*&quot;, full.names = TRUE) data_ids &lt;- str_extract(data_files, &quot;(tx_\\\\d+)&quot;) %&gt;% str_sub(-3,-1) { # read in release data and get first release - 24 hrs tag_info &lt;- fread(&quot;data/data2018/SelinDB.csv&quot;) tag_info &lt;- tag_info[!is.na(Release_Date) &amp; !is.na(Release_Time),] tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = &quot; &quot;), format = &quot;%d.%m.%y %H:%M&quot;, tz = &quot;CET&quot;)] } data_files &lt;- data_files[as.integer(data_ids) %in% tag_info$Toa_Tag] # prepare recurse data folder if(!dir.exists(&quot;data/data2018/revisitData&quot;)){ dir.create(&quot;data/data2018/revisitData&quot;) } # prepare recurse in parameters # radius (m), cutoff (mins) {radius &lt;- 50 timeunits &lt;- &quot;mins&quot; revisit_cutoff &lt;- 60} # map read in, splitting, and recurse over individual level data # remove visits where the bird left for 60 mins, and then returned # this is regardless of whether after its return it stayed there # the removal counts the cumulative sum of all (timeSinceLastVisit &lt;= 60) # thus after the first 60 minute absence, all points are assigned TRUE # this must be grouped by the coordinate map(data_files, function(df){ # read in, fix data type, and split temp_data &lt;- fread(df, integer64 = &quot;numeric&quot;) temp_data[,ts:=fastPOSIXct(ts, tz = &#39;CET&#39;)] setDF(temp_data) temp_data &lt;- split(temp_data, temp_data$tide_number) # map over the tidal cycle level data map(temp_data, function(tempdf){ # perform the recursion analysis df_recurse &lt;- getRecursions(x = tempdf[,c(&quot;x&quot;,&quot;y&quot;,&quot;ts&quot;,&quot;id&quot;)], radius = radius, timeunits = timeunits, verbose = TRUE) # extract revisit statistics and calculate residence time # and revisits with a 1 hour cutoff df_recurse &lt;- setDT(df_recurse[[&quot;revisitStats&quot;]]) df_recurse[,timeSinceLastVisit:= ifelse(is.na(timeSinceLastVisit), -Inf, timeSinceLastVisit)] df_recurse[,longAbsenceCounter:= cumsum(timeSinceLastVisit &gt; 60), by= .(coordIdx)] df_recurse &lt;- df_recurse[longAbsenceCounter &lt; 1,] df_recurse &lt;- df_recurse[,.(resTime = sum(timeInside), fpt = first(timeInside), revisits = max(visitIdx)), by=.(coordIdx,x,y)] # prepare and merge existing data with recursion data setDT(tempdf)[,coordIdx:=1:nrow(tempdf)] tempdf &lt;- merge(tempdf, df_recurse, by = c(&quot;x&quot;, &quot;y&quot;, &quot;coordIdx&quot;)) setorder(tempdf, ts) # write each data frame to file fwrite(tempdf, file = glue(&#39;data/data2018/revisitData/{unique(tempdf$id)}_{str_pad(unique(tempdf$tide_number), width=3, pad=&quot;0&quot;)}_revisit.csv&#39;)) message(glue(&#39;recurse {unique(tempdf$id)}_{str_pad(unique(tempdf$tide_number), width=3, pad=&quot;0&quot;)} done&#39;)) rm(tempdf, df_recurse) }) }) "],
["residence-patch-construction.html", "6 Residence patch construction 6.1 Prepare libraries 6.2 Patch construction 6.3 Get patch trajectories", " 6 Residence patch construction This section is about using the main watlasUtils functions to infer residence points when data is missing from a movement track, to classify points into residence or travelling, and to construct low-tide residence patches from the residence points. Summary statistics on these spatial outputs are then exported to file for further use. Workflow Prepare watlasUtils and required libraries, Read data, infer residence, classify points, construct low-tide patches, and get spatial data from patches, Get patch trajectories, Export spatial data. 6.1 Prepare libraries # load watlasUtils or install if not available if(&quot;watlasUtils&quot; %in% installed.packages() == FALSE){ devtools::install_github(&quot;pratikunterwegs/watlasUtils&quot;) } library(watlasUtils) # libraries to process data library(data.table) library(purrr) library(stringr) library(glue) library(readr) library(dplyr) library(fasttime) # libraries for the cluster library(ssh) 6.2 Patch construction Connect to cluster and process data up to and beyond patches. # read password password = read_csv(&quot;data/password.txt&quot;)$password # get bird ids data_ids &lt;- str_extract(data_files, &quot;(tx_\\\\d+)&quot;) %&gt;% str_sub(-3,-1) # make a list of data files to read data_files &lt;- list.files(path = &quot;data/data2018/revisitData&quot;, pattern = &quot;_revisit.csv&quot;, full.names = TRUE) # map inferResidence, classifyPath, and getPatches over data output_data &lt;- map(data_files, function(df){ # connect to peregrine and transfer data { s &lt;- ssh_connect(&quot;p284074@peregrine.hpc.rug.nl&quot;, passwd = password) # make directory if non-existent ssh_exec_wait(s, command = &quot;mkdir -p data/watlas_2018&quot;) # list files already present files_on_prg &lt;- ssh_exec_internal(s, command = &quot;ls data/watlas_2018&quot;) files_on_prg &lt;- rawToChar(files_on_prg$stdout) %&gt;% str_split(&quot;\\n&quot;) %&gt;% unlist() # check name data_name &lt;- df %&gt;% str_split(&quot;/&quot;) %&gt;% unlist() %&gt;% .[3] if(!data_name %in% files_on_prg){ # upload data file for processing scp_upload(s, df, to = &quot;data/watlas_2018/&quot;) } } # get id tide combination id_tide &lt;- as.character(glue(&#39;{unique(df$id)}_{unique(df$tide_number)}&#39;)) # make job file { shebang &lt;- readLines(&quot;code/template_job_patches.sh&quot;) # rename job shebang[2] &lt;- glue(&#39;#SBATCH --job-name=patches_{id_tide}&#39;) text &lt;- glue(&#39;Rscript code/code_process_patches.r {df}&#39;) jobfile &lt;- glue(&#39;code/job_patches_{id_tide}.sh&#39;) writeLines(c(shebang, text), con = jobfile) scp_upload(s, jobfile, to = &quot;code/&quot;) } ssh_exec_wait(s, command = glue(&#39;dos2unix {jobfile}&#39;)) # process using ctmm ssh_exec_wait(s, command = glue(&#39;sbatch {jobfile}&#39;)) # disconnect ssh_disconnect(s) }) 6.3 Get patch trajectories # save as temp data save(output_data, file = &quot;data/data2018/patch_data_2018.rdata&quot;) # filter non-sf objects from the list output_data &lt;- keep(output_data, function(obj){&quot;data.frame&quot; %in% class(obj)}) output_data &lt;- bind_rows(output_data) fwrite(output_data, file = &quot;data/data2018/patch_summary.csv&quot;, dateTimeAs = &quot;ISO&quot;) "],
["linking-patch-metrics-to-personality.html", "7 Linking patch metrics to personality 7.1 Prepare libraries 7.2 Read in exploration score 7.3 Exploratory analysis on patch metrics 7.4 Patch metrics as an outcome of exploration score 7.5 Plot model figures", " 7 Linking patch metrics to personality This section is about linking patch metrics to individual personality measures, using the previously derived patch summary data, and experimentally measured exploration scores as a proxy for personality. Workflow Prepare required libraries, Link patch metrics with individual behaviour score, Run models for patch metrics of interest, Plot figures for metrics ~ personality, Export figures and model output. 7.1 Prepare libraries # libraries to process data library(readr) # using readr here since data is small library(purrr) library(glue) library(dplyr) library(tidyr) # libraries for stats library(lmerTest) # libraries for plotting library(ggplot2) library(ggthemes) library(scico) library(scales) library(gridExtra) library(cowplot) # ci func # simple ci function ci &lt;- function(x){ qnorm(0.975)*sd(x, na.rm = T)/sqrt((length(x)))} 7.2 Read in exploration score # read in tag info tag_info &lt;- read_csv(&quot;data/data2018/SelinDB.csv&quot;) %&gt;% select(RINGNR, Toa_Tag) %&gt;% filter(!is.na(Toa_Tag)) # read in updated behavioural data behav_score &lt;- read_delim(&quot;data/data2018/Selindb-updated_2019-07-17.csv&quot;, delim = &quot;;&quot;) behav_score &lt;- inner_join(tag_info, behav_score, by = c(&quot;RINGNR&quot; = &quot;FB&quot;)) %&gt;% filter(trial == &quot;F01&quot;) %&gt;% select(Toa_Tag, texpl) 7.3 Exploratory analysis on patch metrics # read in patch data patch_data &lt;- read_csv(&quot;data/data2018/patch_summary.csv&quot;) # process data to plot histograms of area, duration, circularity, displacement # propfixes and tidal time patch_data &lt;- patch_data %&gt;% select(id, tide_number, area, circularity, duration, dispInPatch, propfixes, tidaltime_mean, distBwPatch) %&gt;% pivot_longer(cols = c(area, circularity, duration, dispInPatch, propfixes, tidaltime_mean, distBwPatch), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) # filter out 95% values in each variable patch_data &lt;- patch_data %&gt;% group_by(variable) %&gt;% mutate(scaled_val = rescale(value)) %&gt;% filter(scaled_val &lt; 0.90) # plot histograms ggplot(patch_data)+ geom_histogram(aes(x = value))+ facet_wrap(~variable, scales = &quot;free&quot;)+ theme_bw() 7.4 Patch metrics as an outcome of exploration score # read in patch data patch_data &lt;- read_csv(&quot;data/data2018/patch_summary.csv&quot;) # filter for patch quality on the proportion of expected fixes acheived patch_data &lt;- filter(patch_data, propfixes %between% c(0.2, 1.0)) # join to behav data patch_data &lt;- inner_join(patch_data, behav_score, by=c(&quot;id&quot; = &quot;Toa_Tag&quot;)) %&gt;% mutate(texpl = as.numeric(texpl)) %&gt;% filter(!is.na(texpl)) # prepare for simultaneous modelling model_data &lt;- setDF(patch_data) %&gt;% pivot_longer(names_to = &quot;response_variable&quot;, values_to = &quot;response_value&quot;, cols = c(duration, dispInPatch, distBwPatch, area)) %&gt;% group_by(response_variable) %&gt;% nest() # run models using glmerTest and get predict output model_data &lt;- mutate(model_data, model = map(data, function(z){ lmer(response_value ~ texpl + (1|tide_number) + tidaltime_mean + I((tidaltime_mean)^2), data = z, na.action = na.omit)})) %&gt;% mutate(predMod = map2(model, data, function(a, b){ mutate(b, predval = predict(a, type = &quot;response&quot;, newdata = b, allow.new.levels = TRUE)) })) # assign names to model object names(model_data$model) &lt;- glue(&quot;response = {model_data$response_variable} | predictor = tExplScore&quot;) 7.5 Plot model figures # prepare data for plotting plot_data &lt;- mutate(model_data, data_expl = map(data, function(df){ select(df, texpl, response_value, tide_number) %&gt;% filter(tide_number &lt; 100) %&gt;% mutate(texpl = plyr::round_any(texpl, 0.1)) %&gt;% group_by(texpl) %&gt;% summarise_at(vars(response_value), .funs = c(~mean(., na.rm=T), ~ci(.)))})) # prepare plots to arrange plots &lt;- plot_data %&gt;% ungroup() %&gt;% mutate(y = c(&quot;time in patch (mins)&quot;, &quot;disp in patch (m)&quot;, &quot;dist b/w patch (m)&quot;, &quot;patch area (m.sq.)&quot;), title = glue::glue(&#39;{letters[1:4]} {y}&#39;)) %&gt;% # plots column mutate(plot = pmap(select(., data_expl, y, title), function(data_expl, y, title){ ggplot(data_expl)+ geom_pointrange(aes(x = texpl, y = mean, ymin = mean-ci, ymax = mean+ci), position = position_dodge(width = 0.01))+ scale_y_continuous(labels = scales::comma)+ # facet_grid(~phase, scales = &quot;free_x&quot;)+ #scico::scale_colour_scico(palette = &quot;cork&quot;)+ # # coord_cartesian(xlim = c(0,1), # ylim = c(quantile(data_expl$mean, 0.00, na.rm = TRUE), # quantile(data_expl$mean, 0.999, na.rm = TRUE)))+ scale_x_continuous(breaks = c(0,0.5,1))+ theme_few()+ theme(legend.position = &quot;none&quot;, #plot.background = element_rect(colour = 1), #plot.margin = unit(rep(0.5, 4), &quot;cm&quot;), #axis.text.y = element_text(angle = 90, vjust = 0.5), plot.title = element_text(face = &quot;bold&quot;))+ labs(y = y, title = title, x = &quot;exploration score&quot;) }), plot = map(plot, cowplot::as_grob)) # arrange plots plotlist = plots$plot { # pdf(file = &quot;../figs/fig05patchMetrics.pdf&quot;, width = 8, height = 12) grid.arrange(grobs = plotlist, ncol = 2) # dev.off() } "],
["applying-ctmm-to-watlas-data.html", "8 Applying CTMM to WATLAS data 8.1 Load libraries 8.2 Load a random subset of data 8.3 Choose scales of aggregation", " 8 Applying CTMM to WATLAS data CTMM (Calabrese, Fleming, and Gurarie 2016) can estimate speed from poor quality data. However, CTMM has its limits, and fails most frequently when the movement model is identified as being fractal. A solution to this is to decrease the temporal resolution of the data within residence patches, i.e., to aggregate positions over a certain interval. However, data resulting from this procedure may run afoul of another CTMM feature, outlier removal, which may recommend the removal of nearly all points in a trajectory. This depends on the percentile of the data (in terms of speed and distance from the centroid(?)) considered to be outliers, as well as on the scale of aggregation used earlier to avoid fractal movement models. Here, we examine the effect of different aggregation scales on the proportion of patches that would be discarded. 8.1 Load libraries # load libs library(data.table) library(lubridate) library(sf) library(glue) library(stringr) library(fasttime) library(tibble) library(dplyr) library(purrr) library(tidyr) # devtools::install_github(&quot;pratikunterwegs/watlasUtils&quot;, ref = &quot;devbranch&quot;) library(watlasUtils) library(ctmm) # plot libs library(ggplot2) library(ggthemes) 8.2 Load a random subset of data # make a list of data files to read data_files &lt;- list.files(path = &quot;data/data2018/revisitData&quot;, pattern = &quot;_revisit.csv&quot;, full.names = TRUE) # select a random subset of around 1% data_files &lt;- data_files[runif(length(data_files)) &lt; 0.01] 8.3 Choose scales of aggregation scales &lt;- c(15, 30, 60) data_to_test &lt;- crossing(scales, data_files) rm(data_files, scales) some_output &lt;- pmap(data_to_test, function(scales, data_files){ { temp_data &lt;- fread(data_files[[1]]) temp_data[,ts:=fastPOSIXct(ts)] id &lt;- unique(temp_data$id) tide_number &lt;- unique(temp_data$tide_number) # wrap process in try catch patch_points = tryCatch( { # watlasUtils function to infer residence temp_data &lt;- wat_infer_residence(df = temp_data, infResTime = 2, infPatchTimeDiff = 30, infPatchSpatDiff = 100) # watlasUtils function to classify path temp_data &lt;- wat_classify_points(somedata = temp_data, resTimeLimit = 2) # watlasUtils function to get patches patch_data &lt;- wat_make_res_patch(somedata = temp_data, bufferSize = 10, spatIndepLim = 100, tempIndepLim = 30, restIndepLim = 30, minFixes = 3, tideLims = c(4,10)) # get patch data points patch_points &lt;- wat_get_patch_summary(resPatchData = patch_data, dataColumn = &quot;data&quot;, whichData = &quot;points&quot;) }, # null error function, with option to collect data on errors error= function(e) { message(glue::glue(&#39;patches {id}_{tide_number} errored&#39;)) } ) } if(&quot;data.frame&quot; %in% class(patch_points)){ ratio &lt;- tryCatch({ # prepare for telemetry { data_for_ctmm &lt;- setDT(patch_points)[,.(id, tide_number, x, y, patch, time, VARX, VARY)] # aggregate within a patch to 10 seconds data_for_ctmm &lt;- split(data_for_ctmm, f = data_for_ctmm$patch) data_for_ctmm &lt;- map(data_for_ctmm, wat_agg_data, interval = 15) %&gt;% keep(function(z) nrow(z) &gt; 10) %&gt;% bind_rows() # make each patch an indiv setDT(data_for_ctmm) data_for_ctmm[,individual.local.identifier:= paste(id, tide_number, patch, sep = &quot;_&quot;)] # get horizontal error data_for_ctmm[,HDOP := sqrt(VARX+VARY)/10] # subset columns data_for_ctmm &lt;- data_for_ctmm[,.(individual.local.identifier, time, x, y, HDOP)] # get new names setnames(data_for_ctmm, old = c(&quot;x&quot;, &quot;y&quot;, &quot;time&quot;), new = c(&quot;UTM.x&quot;,&quot;UTM.y&quot;, &quot;timestamp&quot;)) # convert time to posixct data_for_ctmm[,timestamp:=as.POSIXct(timestamp, origin = &quot;1970-01-01&quot;)] # add UTM zone data_for_ctmm[,zone:=&quot;31 +north&quot;] } # make telemetry { tel &lt;- as.telemetry(data_for_ctmm) } n_patches &lt;- length(tel) # ctmm section { # get the outliers but do not plot outliers &lt;- map(tel, outlie, plot=FALSE) # get a list of 99 th percentile outliers q90 &lt;- map(outliers, function(this_outlier_set){ quantile(this_outlier_set[[1]], probs = c(0.99)) }) # remove outliers from telemetry data tel &lt;- pmap(list(tel, outliers, q90), function(this_tel_obj, this_outlier_set, outlier_quantile) {this_tel_obj[-(which(this_outlier_set[[1]] &gt;= outlier_quantile)),]}) # some patches have no data remaining, filter them out tel &lt;- keep(tel, function(this_tel){nrow(this_tel) &gt; 0}) r_patches &lt;- length(tel) } ratio = r_patches/n_patches message(ratio) return(ratio) }, error= function(e) { #return(NA) }) } }) # get ratios data_to_test$ratio &lt;- map_if(.x = some_output, .p = is.null, .f = function(l){NA}) %&gt;% unlist() # plot ratios fig_patch_thinning &lt;- ggplot(data_to_test)+ geom_jitter(aes(x = factor(scales), y = ratio, group = scales, col = factor(scales), shape = factor(scales)), size = 2, stroke = 1)+ scale_fill_brewer()+ scale_shape_few()+ theme_few()+ theme(legend.position = &quot;none&quot;)+ labs(x = &quot;aggregation interval (s)&quot;, y = &quot;% patches retained&quot;) # save to figs ggsave(plot = fig_patch_thinning, filename = &quot;figs/fig_patch_thinning.png&quot;) knitr::include_graphics(&quot;figs/fig_patch_thinning.png&quot;) Bracis, Chloe, Keith L. Bildstein, and Thomas Mueller. 2018. “Revisitation Analysis Uncovers Spatio-Temporal Patterns in Animal Movement Data.” Ecography 41 (11): 1801–11. https://doi.org/10.1111/ecog.03618. Calabrese, Justin M., Chris H. Fleming, and Eliezer Gurarie. 2016. “Ctmm: An R Package for Analyzing Animal Relocation Data as a Continuous-Time Stochastic Process.” Methods in Ecology and Evolution 7 (9): 1124–32. https://doi.org/10.1111/2041-210X.12559. Troy D. Hill, Shimon C. Anisfeld. 2014. VulnToolkit: R for Coastal Vulnerability Analysis. New Haven, CT: Yale School of Forestry; Environmental Studies. https://github.com/troyhill/VulnToolkit. "]
]
