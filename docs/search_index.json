[
["index.html", "Assortative association in a migratory Arctic wader Section 1 Introduction 1.1 Attribution 1.2 Data access 1.3 Data processing", " Assortative association in a migratory Arctic wader Pratik R Gupte 2020-07-03 Section 1 Introduction This repository is the source for a project to idenify residence patches from high-resolution tracking data from individual red knots Calidris canutus islandica, and study whether associations are assortative on individual traits. 1.1 Attribution Please contact the following before cloning or in case of interest in the project. Pratik Gupte (author and maintainer) PhD student, GELIFES – University of Groningen Guest researcher, COS – NIOZ p.r.gupte@rug.nl Nijenborgh 7/5172.0583 9747AG Groningen 1.2 Data access The data used in this work are not publicly available. Contact PI Allert Bijleveld (COS-NIOZ) for data access. 1.3 Data processing The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar. "],
["getting-data.html", "Section 2 Getting data 2.1 Prepare watlastools and other libraries 2.2 Read in tag deployment data 2.3 Get data and save locally", " Section 2 Getting data This section focusses on accessing and downloading WATLAS data. This is done using functions in the WATLAS Utilities package. Workflow Preparing required libraries. Reading tag data with deployment start dates from a local file. This file is not yet publicly available. Connecting to the NIOZ databse and downloading data. This database is also not public-access. 2.1 Prepare watlastools and other libraries # install the package watlastools from master branch using the following # install.packages(&quot;devtools&quot;) library(devtools) # devtools::install_github(&quot;pratikunterwegs/watlastools&quot;) library(watlastools) # libraries to process data library(data.table) library(ggplot2) library(ggthemes) library(purrr) library(glue) 2.2 Read in tag deployment data # read deployment data from local file in data folder tag_info &lt;- fread(&quot;data/data2018/SelinDB.csv&quot;) # filter out NAs in release date and time tag_info &lt;- tag_info[!is.na(Release_Date) &amp; !is.na(Release_Time),] # make release date column as POSIXct tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = &quot; &quot;), format = &quot;%d.%m.%y %H:%M&quot;, tz = &quot;CET&quot;)] # check new release date column head(tag_info$Release_Date) 2.3 Get data and save locally # read in database access parameters from a local file data_access &lt;- fread(&quot;data/access_params.txt&quot;) # create a data storage file if not present # use the getData function from watlastools on the tag_info data frame # this is placed inside a pmap wrapper to automate access for all birds if(!dir.exists(&quot;data/data2018&quot;)) { dir.create(&quot;data/data2018&quot;) } pmap(tag_info[,.(Toa_Tag, Release_Date)], function(Toa_Tag, Release_Date){ prelim_data &lt;- watlastools::wat_get_data(tag = Toa_Tag, tracking_time_start = as.character(Release_Date), tracking_time_end = &quot;2018-10-31&quot;, username = data_access$username, password = data_access$password) setDT(prelim_data) # prelim_data[,TAG:= = as.numeric(TAG) - prefix_num] message(glue(&#39;tag {Toa_Tag} accessed with {nrow(prelim_data)} fixes&#39;)) fwrite(prelim_data, file = glue(&#39;data/data2018/{Toa_Tag}_data.csv&#39;), dateTimeAs = &quot;ISO&quot;) }) "],
["cleaning-data.html", "Section 3 Cleaning data 3.1 Prepare watlastools and other libraries 3.2 Prepare to remove attractor points 3.3 Read, clean, and write data 3.4 Tracking period for each id", " Section 3 Cleaning data This section is about cleaning downloaded data using the cleanData function in the WATLAS Utilities package. Workflow Prepare required libraries. Read in data, apply the cleaning function, and overwrite local data. 3.1 Prepare watlastools and other libraries # watlastools assumed installed from the previous step # if not, install from the github repo as shown below devtools::install_github(&quot;pratikunterwegs/watlastools&quot;) library(watlastools) # libraries to process data library(data.table) library(purrr) library(glue) library(fasttime) library(bit64) library(stringr) 3.2 Prepare to remove attractor points # read in identified attractor points atp &lt;- fread(&quot;data/data2018/attractor_points.txt&quot;) 3.3 Read, clean, and write data # make a list of data files to read data_files &lt;- list.files(path = &quot;data/data2018/locs_raw&quot;, pattern = &quot;whole_season*&quot;, full.names = TRUE) data_ids &lt;- str_extract(data_files, &quot;(tx_\\\\d+)&quot;) %&gt;% str_sub(-3,-1) # read deployment data from local file in data folder tag_info &lt;- fread(&quot;data/data2018/SelinDB.csv&quot;) # filter out NAs in release date and time tag_info &lt;- tag_info[!is.na(Release_Date) &amp; !is.na(Release_Time),] # make release date column as POSIXct tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = &quot; &quot;), format = &quot;%d.%m.%y %H:%M&quot;, tz = &quot;CET&quot;)] # sub for knots in data data_files &lt;- data_files[as.integer(data_ids) %in% tag_info$Toa_Tag] # map read in, cleaning, and write out function over vector of filenames map(data_files, function(df){ temp_data &lt;- fread(df, integer64 = &quot;numeric&quot;) # filter for release date + 24 hrs { temp_id &lt;- str_sub(temp_data[1, TAG], -3, -1) rel_date &lt;- tag_info[Toa_Tag == temp_id, Release_Date] temp_data &lt;- temp_data[TIME/1e3 &gt; as.numeric(rel_date + (24*3600)),] } tryCatch( { temp_data &lt;- wat_rm_attractor(df = temp_data, atp_xmin = atp$xmin, atp_xmax = atp$xmax, atp_ymin = atp$ymin, atp_ymax = atp$ymax) clean_data &lt;- wat_clean_data(somedata = temp_data, moving_window = 3, nbs_min = 0, sd_threshold = 100, filter_speed = TRUE, speed_cutoff = 150) agg_data &lt;- wat_agg_data(df = clean_data, interval = 30) message(glue(&#39;tag {unique(agg_data$id)} cleaned with {nrow(agg_data)} fixes&#39;)) fwrite(x = agg_data, file = glue(&#39;data/data2018/locs_proc/id_{temp_id}.csv&#39;), dateTimeAs = &quot;ISO&quot;) rm(temp_data, clean_data, agg_data) }, error = function(e){ message(glue(&#39;tag {unique(temp_id)} failed&#39;)) }) }) 3.4 Tracking period for each id data_files &lt;- list.files(&quot;data/data2018/locs_proc&quot;, full.names = TRUE) total_tracking &lt;- map_df(data_files, function(x){ a &lt;- fread(x) a &lt;- a[c(1, nrow(a)),.(id, time)] a[,event:=c(&quot;time_start&quot;, &quot;time_end&quot;)] }) # get tracking interval for each total_tracking &lt;- tidyr::pivot_wider(total_tracking, id_cols = &quot;id&quot;, values_from = &quot;time&quot;, names_from = &quot;event&quot;) # write to file fwrite(total_tracking, file = &quot;data/data2018/data_2018_id_tracking_interval.csv&quot;) "],
["adding-tidal-cycle-data.html", "Section 4 Adding tidal cycle data 4.1 Prepare libraries 4.2 Read water level data 4.3 Calculate high tides 4.4 Add time since high tide", " Section 4 Adding tidal cycle data This section is about adding tidal cycle data to individual trajectories. This is done to split the data up into convenient, and biologically sensible units. This section uses the package VulnToolkit (Troy D. Hill, Shimon C. Anisfeld 2014) to identify high tide times from water-level data provided by Rijkswaterstaat for the measuring point at West Terschelling. Workflow Prepare required libraries, Read in water level data and identify high tides, Write tidal cycle data to local file, Add time since high tide to movement data. 4.1 Prepare libraries # load VulnToolkit or install if not available if(&quot;VulnToolkit&quot; %in% installed.packages() == FALSE){ devtools::install_github(&quot;troyhill/VulnToolkit&quot;) } library(VulnToolkit) # libraries to process data library(data.table) library(purrr) library(glue) library(dplyr) library(stringr) library(fasttime) library(lubridate) library(watlastools) 4.2 Read water level data Water level data for West Terschelling, a settlement approx. 10km from the field site are provided by Rijkswaterstaat’s Waterinfo, in cm above Amsterdam Ordnance Datum. These data are manually downloaded in the range July 1, 2018 – October 31, 2018 and saved in data/data2018. # read in waterlevel data waterlevel &lt;- fread(&quot;data/data2018/waterlevelWestTerschelling.csv&quot;, sep = &quot;;&quot;) # select useful columns and rename waterlevel &lt;- waterlevel[,.(WAARNEMINGDATUM, WAARNEMINGTIJD, NUMERIEKEWAARDE)] setnames(waterlevel, c(&quot;date&quot;, &quot;time&quot;, &quot;level&quot;)) # make a single POSIXct column of datetime waterlevel[,dateTime := as.POSIXct(paste(date, time, sep = &quot; &quot;), format = &quot;%d-%m-%Y %H:%M:%S&quot;, tz = &quot;CET&quot;)] waterlevel &lt;- setDT(distinct(setDF(waterlevel), dateTime, .keep_all = TRUE)) 4.3 Calculate high tides A tidal period of 12 hours 25 minutes is taken from Rijkswaterstaat. # use the HL function from vulnToolkit to get high tides tides &lt;- VulnToolkit::HL(waterlevel$level, waterlevel$dateTime, period = 12.41, tides = &quot;H&quot;, semidiurnal = TRUE) # read in release data and get first release - 24 hrs tag_info &lt;- fread(&quot;data/data2018/SelinDB.csv&quot;) tag_info &lt;- tag_info[!is.na(Release_Date) &amp; !is.na(Release_Time),] tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = &quot; &quot;), format = &quot;%d.%m.%y %H:%M&quot;, tz = &quot;CET&quot;)] first_release &lt;- min(tag_info$Release_Date) - (3600*24) # remove tides before first release tides &lt;- setDT(tides)[time &gt; first_release, ][,tide2:=NULL] tides[,tide_number:=1:nrow(tides)] # write to local file fwrite(tides, file = &quot;data/data2018/tidesSummer2018.csv&quot;, dateTimeAs = &quot;ISO&quot;) 4.4 Add time since high tide # read in data and add time since high tide data_files &lt;- list.files(path = &quot;data/data2018/locs_proc&quot;, pattern = &quot;id_&quot;, full.names = TRUE) data_ids &lt;- str_extract(data_files, &quot;(id_\\\\d+)&quot;) %&gt;% str_sub(-3,-1) # map read in and tidal time calculation over data # merge data to insert high tides within movement data # arrange by time to position high tides correctly map(data_files, function(df){ # read and fix data types temp_data &lt;- fread(df, integer64 = &quot;numeric&quot;) temp_data[,ts:=fastPOSIXct(ts, tz = &quot;CET&quot;)] # merge with tides and order on time temp_data &lt;- wat_add_tide(df = temp_data, tide_data = &quot;data/data2018/tidesSummer2018.csv&quot;) # add waterlevel temp_data[,temp_time := lubridate::round_date(ts, unit = &quot;10 minute&quot;)] temp_data &lt;- merge(temp_data, waterlevel[,.(dateTime, level)], by.x = &quot;temp_time&quot;, by.y = &quot;dateTime&quot;) setnames(temp_data, old = &quot;level&quot;, new = &quot;waterlevel&quot;) # export data, print msg, remove data fwrite(temp_data, file = df, dateTimeAs = &quot;ISO&quot;) message(glue(&#39;tag {unique(temp_data$id)} added time since high tide&#39;)) rm(temp_data) }) References "],
["revisit-analysis.html", "Section 5 Revisit analysis 5.1 Prepare libraries 5.2 Read data, split, recurse, write", " Section 5 Revisit analysis This section is about splitting the data by tidal cycle, and passing the individual- and tidal cycle-specific data to revisit analysis, which is implemented using the package recurse. Workflow Prepare required libraries, Performing recurse: Read in movement data and split by tidal cycle, Perform revisit analysis using recurse, Write data with revisit metrics to file. 5.1 Prepare libraries This section uses the recurse package (Bracis, Bildstein, and Mueller 2018). # load recurse or install if not available if(&quot;recurse&quot; %in% installed.packages() == FALSE){ install.packages(&quot;recurse&quot;) } library(recurse) # libraries to process data library(data.table) library(purrr) library(glue) library(dplyr) library(fasttime) library(stringr) 5.2 Read data, split, recurse, write # read in data data_files &lt;- list.files(path = &quot;data/data2018/&quot;, pattern = &quot;id_&quot;, full.names = TRUE) data_ids &lt;- str_extract(data_files, &quot;(id_\\\\d+)&quot;) %&gt;% str_sub(-3,-1) { # read in release data and get first release - 24 hrs tag_info &lt;- fread(&quot;data/data2018/SelinDB.csv&quot;) tag_info &lt;- tag_info[!is.na(Release_Date) &amp; !is.na(Release_Time),] tag_info[,Release_Date := as.POSIXct(paste(Release_Date, Release_Time, sep = &quot; &quot;), format = &quot;%d.%m.%y %H:%M&quot;, tz = &quot;CET&quot;)] } # prepare recurse data folder if(!dir.exists(&quot;data/data2018/revisitData&quot;)){ dir.create(&quot;data/data2018/revisitData&quot;) } # prepare recurse in parameters # radius (m), cutoff (mins) { radius &lt;- 50 timeunits &lt;- &quot;mins&quot; revisit_cutoff &lt;- 60 } # map read in, splitting, and recurse over individual level data # remove visits where the bird left for 60 mins, and then returned # this is regardless of whether after its return it stayed there # the removal counts the cumulative sum of all (timeSinceLastVisit &lt;= 60) # thus after the first 60 minute absence, all points are assigned TRUE # this must be grouped by the coordinate map(data_files, function(df){ # read in, fix data type, and split temp_data &lt;- fread(df, integer64 = &quot;numeric&quot;) temp_data[,ts:=fastPOSIXct(ts, tz = &#39;CET&#39;)] setDF(temp_data) temp_data &lt;- split(temp_data, temp_data$tide_number) # map over the tidal cycle level data map(temp_data, function(tempdf){ tryCatch({ # perform the recursion analysis df_recurse &lt;- getRecursions(x = tempdf[,c(&quot;x&quot;,&quot;y&quot;,&quot;ts&quot;,&quot;id&quot;)], radius = radius, timeunits = timeunits, verbose = TRUE) # extract revisit statistics and calculate residence time # and revisits with a 1 hour cutoff df_recurse &lt;- setDT(df_recurse[[&quot;revisitStats&quot;]]) df_recurse[,timeSinceLastVisit:= ifelse(is.na(timeSinceLastVisit), -Inf, timeSinceLastVisit)] df_recurse[,longAbsenceCounter:= cumsum(timeSinceLastVisit &gt; 60), by= .(coordIdx)] df_recurse &lt;- df_recurse[longAbsenceCounter &lt; 1,] df_recurse &lt;- df_recurse[,.(resTime = sum(timeInside), fpt = first(timeInside), revisits = max(visitIdx)), by=.(coordIdx,x,y)] # prepare and merge existing data with recursion data setDT(tempdf)[,coordIdx:=1:nrow(tempdf)] tempdf &lt;- merge(tempdf, df_recurse, by = c(&quot;x&quot;, &quot;y&quot;, &quot;coordIdx&quot;)) setorder(tempdf, ts) # write each data frame to file fwrite(tempdf, file = glue(&#39;data/data2018/revisitData/{unique(tempdf$id)}_{str_pad(unique(tempdf$tide_number), width=3, pad=&quot;0&quot;)}_revisit.csv&#39;)) message(glue(&#39;recurse {unique(tempdf$id)}_{str_pad(unique(tempdf$tide_number), width=3, pad=&quot;0&quot;)} done&#39;)) rm(tempdf, df_recurse) }, error = function(e){ message(&quot;some recurses failed&quot;) }) }) }) References "],
["residence-patch-construction.html", "Section 6 Residence patch construction 6.1 Prepare libraries 6.2 Patch construction", " Section 6 Residence patch construction This section is about using the main watlastools functions to infer residence points when data is missing from a movement track, to classify points into residence or travelling, and to construct low-tide residence patches from the residence points. Summary statistics on these spatial outputs are then exported to file for further use. Workflow Prepare watlastools and required libraries, Read data, infer residence, classify points, construct patches, repair patches, and write movement data and patch summary to file. 6.1 Prepare libraries # load watlastools or install if not available if(&quot;watlastools&quot; %in% installed.packages() == FALSE){ devtools::install_github(&quot;pratikunterwegs/watlastools&quot;) } library(watlastools) # libraries to process data library(dplyr) library(data.table) library(purrr) library(stringr) library(glue) library(readr) library(fasttime) # functions for this stage alone ci &lt;- function(x){qnorm(0.975)*sd(x, na.rm = T)/sqrt((length(x)))} 6.2 Patch construction if(file.exists(&quot;data/data2018/data_2018_patch_summary.csv&quot;)){ file.remove(&quot;data/data2018/data_2018_patch_summary.csv&quot;) } Process patches. Takes approx. 5 hours for 3 second data. # make a vector of data files to read data_files &lt;- list.files(path = &quot;data/data2018/revisitData&quot;, pattern = &quot;_revisit.csv&quot;, full.names = TRUE) # get tag ids data_id &lt;- str_split(data_files, &quot;/&quot;) %&gt;% map(function(l)l[[4]] %&gt;% str_sub(1,3)) %&gt;% flatten_chr() # make df of tag ids and files data &lt;- data_frame(tag = data_id, data_file = data_files) data &lt;- split(x = data, f = data$tag) %&gt;% map(function(l) l$data_file) # map inferResidence, classifyPath, and getPatches over data walk(data, function(df_list){ patch_data &lt;- map(df_list, function(l){ # read the data file temp_data &lt;- fread(l) temp_data[,ts:=fastPOSIXct(ts)] id &lt;- unique(temp_data$id) tide_number &lt;- unique(temp_data$tide_number) # wrap process in try catch tryCatch( { # watlastools function to infer residence temp_data &lt;- wat_infer_residence(df = temp_data, infPatchTimeDiff = 30, infPatchSpatDiff = 100) # watlastools function to classify path temp_data &lt;- wat_classify_points(somedata = temp_data, resTimeLimit = 2) # watlastools function to get patches patch_dt &lt;- wat_make_res_patch(somedata = temp_data, bufferSize = 10, spatIndepLim = 100, tempIndepLim = 30, restIndepLim = 30, minFixes = 3) # print message message(as.character(glue(&#39;patches {id}_{tide_number} done&#39;))) return(patch_dt) }, # null error function, with option to collect data on errors error= function(e) { message(glue::glue(&#39;patches {id}_{tide_number} errored&#39;)) } ) }) tryCatch( { # repair high tide patches across an individual&#39;s tidal cycles repaired_data &lt;- wat_repair_ht_patches(patch_data_list = patch_data) # write patch summary data if(all(is.data.frame(repaired_data), nrow(repaired_data) &gt; 0)){ # watlastools function to get patch data as summary patch_summary &lt;- wat_get_patch_summary(res_patch_data = repaired_data, whichData = &quot;summary&quot;) fwrite(patch_summary, file = &quot;data/data2018/data_2018_patch_summary.csv&quot;, append = TRUE) # we also want the spatial object patch_spatial &lt;- wat_get_patch_summary(res_patch_data = repaired_data, whichData = &quot;spatial&quot;) sf::`st_crs&lt;-`(patch_spatial, 32631) } sf::st_write(patch_spatial, dsn = &quot;data/data2018/spatials/patches_2018.gpkg&quot;, append = TRUE) }, error = function(e){ message(glue::glue(&#39;patch writing errored&#39;)) } ) }) "],
["spatio-temporal-overlap-in-patches.html", "Section 7 Spatio-temporal overlap in patches 7.1 Do patches overlap in time 7.2 How much do patches overlap in time? 7.3 Which patches overlap in space, and where?", " Section 7 Spatio-temporal overlap in patches library(reticulate) use_python(Sys.which(&quot;Python&quot;)) 7.1 Do patches overlap in time # using ncls to get faster time overlap import os import pandas as pd import geopandas as gpd import numpy as np from ncls import NCLS print(os.getcwd()) # read in the SPATIAL data # because the patch data has some spatials missing # ie patches are described but not made patches = gpd.read_file(&quot;data/data2018/spatials/patches_2018.gpkg&quot;) # plot patches for a sanity check # subset = patches.iloc[0:1000] # subset.plot(linewidth=0.5, # column=&#39;id&#39;, # alpha=0.2, # cmap=&#39;tab20b&#39;, edgecolor=&#39;black&#39;) # convert to dataframe, export, and read in again data = pd.DataFrame(patches.drop(columns=&#39;geometry&#39;)) # assign unique patch id data[&#39;uid&#39;] = np.arange(0, data.shape[0]) # overwrite data with uid data.to_csv(&quot;data/data2018/data_2018_patch_summary_has_patches.csv&quot;, index=False) # remove from memory del patches # re-read csv data because of integer handling differences data = pd.read_csv(&quot;data/data2018/data_2018_patch_summary_has_patches.csv&quot;) # get integer series of start and end times of patches t_start = data[&#39;time_start&#39;].astype(np.int64) t_end = data[&#39;time_end&#39;].astype(np.int64) t_id = data[&#39;uid&#39;] # trial ncls # only works on pandas and not geopandas else throws error! # this is very weird behaviour, pd and gpd must differ in int implementation ncls = NCLS(t_start.values, t_end.values, t_id.values) # look at all the overlaps in time # get a dataframe of the overlapping pairs and the extent of overlap data_list = [] for i in np.arange(len(t_id)): ncls = NCLS(t_start[i:].values, t_end[i:].values, t_id[i:].values) it = ncls.find_overlap(t_start[i], t_end[i]) # get the unique patch ids overlapping overlap_id = [] overlap_extent = [] # get the extent of overlap for x in it: overlap_id.append(x[2]) overlap_extent.append(min(x[1], t_end[i]) - max(x[0], t_start[i])) # add the overlap id for each obs uid = [i] * len(overlap_id) # zip the tuples together tmp_data = list(zip(uid, overlap_id, overlap_extent)) # convert to lists tmp_data = list(map(list, tmp_data)) tmp_data = list(filter(lambda x: x[0] != x[1], tmp_data)) # tmp_data = tmp_data[tmp_data.uid != tmp_data.overlap_id] data_list = data_list + tmp_data # concatenate to dataframe data_overlap = pd.DataFrame(data_list, columns=[&#39;uid&#39;, &#39;overlap_id&#39;, &#39;overlap_extent&#39;]) # save data data_overlap.to_csv(&quot;data/data2018/data_time_overlaps_patches_2018.csv&quot;, index=False) 7.2 How much do patches overlap in time? # NEW SECTION # in this section, we quanitify the temporal overlap between individuals # at the global scale, so, how long were two individuals tracked together # read in the data again # group by id and get the first time_start and the final time_end data = pd.read_csv(&quot;data/data2018/data_2018_id_tracking_interval.csv&quot;) # get integer series of start and end times of patches t_start = data[&#39;time_start&#39;].astype(np.int64) t_end = data[&#39;time_end&#39;].astype(np.int64) t_id = data[&#39;id&#39;] # total overlap data_list = [] for i in np.arange(len(t_id)): ncls = NCLS(t_start[i:].values, t_end[i:].values, t_id[i:].values) it = ncls.find_overlap(t_start[i], t_end[i]) # get the unique patch ids overlapping overlap_id = [] overlap_extent = [] # get the extent of overlap for x in it: overlap_id.append(x[2]) overlap_extent.append(min(x[1], t_end[i]) - max(x[0], t_start[i])) # add the overlap id for each obs uid = [t_id[i]] * len(overlap_id) # zip the tuples together tmp_data = list(zip(uid, overlap_id, overlap_extent)) # convert to lists tmp_data = list(map(list, tmp_data)) tmp_data = list(filter(lambda x: x[0] != x[1], tmp_data)) # tmp_data = tmp_data[tmp_data.uid != tmp_data.overlap_id] data_list = data_list + tmp_data # concatenate to dataframe data_overlap = pd.DataFrame(data_list, columns=[&#39;uid&#39;, &#39;overlap_id&#39;, &#39;total_simul_tracking&#39;]) # write total simul tracking data data_overlap.to_csv(&quot;data/data2018/data_2018_id_simul_tracking.csv&quot;, index=False) # wip 7.3 Which patches overlap in space, and where? # import classic python libs import numpy as np # libs for dataframes import pandas as pd import geopandas as gpd # import ckdtree # from scipy.spatial import cKDTree from shapely.geometry import Point, MultiPoint, LineString, MultiLineString, Polygon, MultiPolygon # import ckdtree # from scipy.spatial import cKDTree # import helper functions from helper_functions import simplify_geom, ckd_distance # read in spatial data patches = gpd.read_file(&quot;data/data2018/spatials/patches_2018.gpkg&quot;) patches.head() patches.crs = {&#39;init&#39;: &#39;epsg:32631&#39;} # read in temporal overlaps data_overlap = pd.read_csv(&quot;data/data2018/data_time_overlaps_patches_2018.csv&quot;) # for each overlap uid/overlap_id get the ckd distance of # the corresponding rows in the spatial spatial_cross = [] for i in np.arange(len(data_overlap)): # get the geometries g_a = patches.iloc[data_overlap.iloc[i].uid] g_b = patches.iloc[data_overlap.iloc[i].overlap_id] covers = g_a.geometry.intersects(g_b.geometry) spatial_cross.append(covers) # convert to series and add to data frame data_overlap[&#39;spatial_overlap&#39;] = pd.Series(spatial_cross) # now that we know which patches overlap in space and time # get the extent of overlap, and the actual overlap object data_overlap = data_overlap[spatial_cross] # in a for loop, add the extent and overlap object overlap_extent = [] overlap_obj = [] for i in np.arange(len(data_overlap)): # get the geometries g_a = patches.iloc[data_overlap.iloc[i].uid] g_b = patches.iloc[data_overlap.iloc[i].overlap_id] # get overlap overlap_polygon = g_a.geometry.intersection(g_b.geometry) overlap_obj.append(overlap_polygon) overlap_extent.append(overlap_polygon.area) # add to data data_overlap[&#39;spatial_overlap_area&#39;] = np.asarray(overlap_extent) data_overlap[&#39;geometry&#39;] = overlap_obj # remove spatial overlap col data_overlap = data_overlap.drop(columns=&#39;spatial_overlap&#39;) # make geodataframe overlap_spatials = gpd.GeoDataFrame(data_overlap, geometry=data_overlap[&#39;geometry&#39;]) # save into spatails overlap_spatials.to_file(&quot;data/data2018/spatials/patch_overlap_2018.gpkg&quot;, layer=&#39;overlaps&#39;, driver=&quot;GPKG&quot;) # save to csv data_overlap = pd.DataFrame(overlap_spatials.drop(columns = &#39;geometry&#39;)) data_overlap = data_overlap.rename(columns={&quot;overlap_extent&quot;:&quot;temporal_overlap_seconds&quot;, &quot;uid&quot;:&quot;patch_i_unique_id&quot;, &quot;overlap_id&quot;:&quot;patch_j_unique_id&quot;}) # write to file data_overlap.to_csv(&quot;data/data2018/data_spatio_temporal_overlap_2018.csv&quot;, index=False) # ends here "],
["finding-networks-from-patch-data.html", "Section 8 Finding networks from patch data 8.1 Read in patch data and overlaps 8.2 Retain extreme individuals 8.3 Network construction 8.4 Prepare network data 8.5 Flock attributes and assortativity", " Section 8 Finding networks from patch data Workflow: Read in patch data, and link to individual trait data. Remove individuals with traits within 1 SD of the population mean, and their patches. Find communities in the remaining patches. Filter communities for data quality, and calculate assortativity. Relate assortativity to flock attributes: waterlevel start flock size 8.1 Read in patch data and overlaps # to handle data library(readr) library(scales) library(tidyr); library(tibble) library(magrittr) library(dplyr) library(purrr) library(stringr) # to work with networks library(igraph) # to plot library(ggplot2) library(scico) # read in patch data data_patches &lt;- read_csv(&quot;data/data2018/data_2018_patch_summary_has_patches.csv&quot;) %&gt;% mutate(uid = as.character(1:nrow(.))) # read in overlap data data &lt;- read_csv(&quot;data/data2018/data_spatio_temporal_overlap_2018.csv&quot;) 8.2 Retain extreme individuals 8.2.1 Read in trait data # make nodes data -- this the individual identities # add individual data to patch data data_id &lt;- readxl::read_excel(&quot;data/data2018/Biometrics_2018-2019.xlsx&quot;) %&gt;% filter(str_detect(`TAG NR`, &quot;[a-zA-Z]+&quot;, negate = TRUE)) # a function for gizzard mass get_gizzard_mass &lt;- function(x, y) {-1.09 + (3.78*(x*y))} # add gizzard mass data_id &lt;- mutate(data_id, gizzard_mass = get_gizzard_mass(SH1, SW1)) # rename columns and drop ids without mass and gizzard mass data_id &lt;- data_id %&gt;% select(id = `TAG NR`, wing = WING, mass = MASS, gizzard_mass) %&gt;% distinct(id, .keep_all = TRUE) %&gt;% drop_na(gizzard_mass) # add some exploration scores and tag info data_behav &lt;- read_csv(&quot;data/data2018/2018-19-all_exploration_scores.csv&quot;) %&gt;% filter(Exp == &quot;F01&quot;) data_tag &lt;- read_csv(&quot;data/data2018/tag_info.csv&quot;) %&gt;% mutate(id = as.character(Toa_Tag)) # join all scores data_id &lt;- left_join(data_id, data_tag, by = c(&quot;id&quot;)) %&gt;% left_join(data_behav, by = &quot;FB&quot;) # remove ids with no exploration data_id &lt;- mutate(data_id, behav = Mean) %&gt;% # drop_na(behav) %&gt;% select(id, mass, gizzard_mass, behav) 8.2.2 Plot histogram of traits Plot trait distribution, median, and SD. # melt data data_id_long &lt;- pivot_longer(data_id, cols = -id, names_to = &quot;trait&quot;) data_trait_summary &lt;- data_id_long %&gt;% drop_na() %&gt;% group_by(trait) %&gt;% summarise_at(vars(value), .funs = list(median = median, sd = sd)) # plot data ggplot(data_id_long)+ geom_histogram(aes(value), fill = &quot;grey&quot;)+ geom_vline(data = data_trait_summary, aes(xintercept = median))+ geom_vline(data = data_trait_summary, aes(xintercept = median - sd), lty = 2)+ geom_vline(data = data_trait_summary, aes(xintercept = median + sd), lty = 2)+ facet_wrap(~trait, scales = &quot;free&quot;, switch = &quot;x&quot;)+ theme_bw() ggsave(filename = &quot;figs/fig_trait_distr.png&quot;, dpi = 300) 8.2.3 Keep only extreme individuals Remove individuals with gizzard mass within the median +/- SD value. # get median and SD median_gm &lt;- filter(data_trait_summary, trait == &quot;gizzard_mass&quot;) %&gt;% .$median sd_gm &lt;- filter(data_trait_summary, trait == &quot;gizzard_mass&quot;) %&gt;% .$sd # filter ids data_id &lt;- filter(data_id, gizzard_mass &gt; median_gm + sd_gm) # !between(gizzard_mass, median_gm - sd_gm, median_gm + sd_gm)) 8.2.4 Keep only extreme individuals’ patches data_patches &lt;- filter(data_patches, id %in% data_id$id) 8.2.5 Remove patches with low fixes Each fix corresponds to 30s time. data_patches &lt;- filter(data_patches, nfixes &gt; 3) 8.2.6 Keep only extreme ids’ edges data &lt;- data %&gt;% filter(patch_i_unique_id %in% data_patches$uid, patch_j_unique_id %in% data_patches$uid) 8.3 Network construction 8.3.1 Make unweighted network from overlaps # prepare edgelist data_edges &lt;- select(data, contains(&quot;patch&quot;)) # prepare network from edgelist network_overlap &lt;- igraph::graph_from_data_frame(d = data_edges, directed = F) 8.3.2 Find communities # get communities using fastgreedy # the network is unweighted data_communities &lt;- igraph::fastgreedy.community(network_overlap) d &lt;- communities(data_communities) %&gt;% imap(function(.x, .id){ tibble(flock = .id, patch = .x) }) # get data as a tibble flock_id &lt;- bind_rows(d) 8.3.3 Join flock data to edgelist # join flock and patch by uid on the first patch data &lt;- data %&gt;% mutate_at(vars(matches(&quot;patch&quot;)), as.character) %&gt;% left_join(flock_id, by = c(&quot;patch_i_unique_id&quot; = &quot;patch&quot;)) # join patch data, basically id data, to flock data tmp_patches &lt;- filter(data_patches, uid %in% flock_id$patch) %&gt;% select(id, uid) data &lt;- left_join(data, tmp_patches, by = c(&quot;patch_i_unique_id&quot; = &quot;uid&quot;)) %&gt;% left_join(tmp_patches, by = c(&quot;patch_j_unique_id&quot; = &quot;uid&quot;)) # get id and weights edges_df &lt;- select(data, matches(&quot;(id.)|flock|spatial&quot;)) %&gt;% rename(id_x = id.x, id_y = id.y) %&gt;% filter(id_x != id_y) 8.4 Prepare network data 8.4.1 Prepare subset-wise edgelist Here, the subsets are the tide number and tidal stage. The edge-list indicates pairwise overlap. # group and nest edges_df &lt;- group_by(edges_df, flock) %&gt;% nest() # count flock edges and remove small flocks # ie, with fewer than 3 edges edges_df &lt;- mutate(edges_df, n_edges = map_int(data, nrow)) %&gt;% filter(n_edges &gt; 3) 8.4.2 Prepare nodes data # expand nodes df to match edges data nodes_df &lt;- summarise(edges_df, id = map(data, function(df) {union(df$id_x, df$id_y)})) %&gt;% mutate(id = map(id, function(df){ df &lt;- tibble(id = as.character(df)) %&gt;% left_join(data_id, by = &quot;id&quot;) # remove nodes with no attributes df &lt;- drop_na(df, gizzard_mass) })) 8.4.3 Prepare flock networks Join the edges and nodes data. network_df &lt;- left_join(edges_df, nodes_df) 8.4.4 Weigh edges by number of overlaps Weigh the interactions between id_x and id_y by the number of such interactions among unique pairs of individuals. A continuous option of weighing by summed overlap also exists. network_df &lt;- mutate(network_df, data = map(data, function(df) { df &lt;- count(df, id_x, id_y, name = &quot;associations&quot;) })) Plot a histogram of associations among pairs in different flocks. # get the data data_assoc &lt;- bind_rows(network_df$data) # where are the quantiles quantile(data_assoc$associations, probs = c(1:10/10, 0.99)) ggplot(data_assoc)+ geom_histogram(aes(associations))+ scale_x_sqrt(breaks = c(0:10), labels = c(0:10)) Remove edges where the number of associations is above 4. # remove using a map over data network_df &lt;- mutate(network_df, data = map(data, function(df) { df &lt;- filter(df, associations &lt;= 10) })) # remove any data where there are now no edges network_df &lt;- filter(network_df, map_lgl(data, function(x) { nrow(x) &gt;= 1 })) Some 1213 flocks remain. 8.4.5 Make networks This data needs to be converted to igraph networks. The third column assocations is the edge weight. # prepare network objects for assortnet via igraph network_df &lt;- mutate(network_df, net = map2(data, id, function(e, n){ # exclude edges with no vertex attributes e &lt;- filter(e, id_x %in% n$id, id_y %in% n$id) tmp_net &lt;- igraph::graph_from_data_frame(d = e, directed = FALSE, vertices = n) })) Remove networks with few (&lt; 3) real edges. # check which graphs are (still) okay network_df &lt;- mutate(network_df, n_edges = map_int(net, function(n){ E(n) %&gt;% length() })) %&gt;% ungroup() # remove bad graphs, ie, with fewer than 3 edges # this is both for quality and practical reasons min_edges &lt;- 1 min_id &lt;- 2 network_df &lt;- filter(network_df, n_edges &gt; min_edges) 8.4.6 Get assortativity # get weighred assortativity by gizzard mass network_df &lt;- mutate(network_df, assort_metrics = purrr::map(net, function(n){ adj &lt;- igraph::as_adjacency_matrix(n, sparse = FALSE) assort_gizzard &lt;- assortnet::assortment.continuous(adj, V(n)$gizzard_mass) assort_behav &lt;- assortnet::assortment.continuous(adj, V(n)$behav) assort_mass &lt;- assortnet::assortment.continuous(adj, V(n)$mass) assortment &lt;- map_df(list(assort_gizzard , assort_behav, assort_mass ), as_tibble) %&gt;% mutate(trait = c(&quot;gizzard&quot; , &quot;behav&quot;, &quot;mass&quot; )) })) Unnest the data. # prepare the data plot_df &lt;- select(network_df, flock, id, n_edges, contains(&quot;assort&quot;)) %&gt;% # count flock size mutate(flock_size = map_int(id, nrow)) %&gt;% select(-id) %&gt;% pivot_longer(cols = contains(&quot;assort&quot;)) # unnest plot_df &lt;- unnest(plot_df, &quot;value&quot;) 8.5 Flock attributes and assortativity 8.5.1 Prepare flock data # filter for existing node data nodes_df &lt;- filter(nodes_df, map_lgl(id, function(df){nrow(df)&gt;1})) # unnest flock id flock_data &lt;- nodes_df # add flock data to patch data data_patches &lt;- left_join(data_patches, flock_id, by = c(&quot;uid&quot; = &quot;patch&quot;)) # separate by tidal stage, where waterlevel &lt; 55 is low tide data_flock_attrs &lt;- select(data_patches, uid, matches(&quot;(x|y|waterlevel)(_start)&quot;), flock) %&gt;% filter(uid %in% flock_id$patch) %&gt;% group_by(flock) %&gt;% summarise_at(vars(contains(&quot;start&quot;)), .funs = median) # add waterlevel to flock flock_data &lt;- left_join(flock_data, data_flock_attrs, by = &quot;flock&quot;) flock_data &lt;- mutate(flock_data, tide_stage = if_else(waterlevel_start &lt; 55, &quot;low&quot;, &quot;high&quot;)) 8.5.2 Assortativity in high and low tide # get waterlevel plot_df &lt;- left_join(plot_df, flock_data) ggplot(plot_df)+ geom_point(aes(waterlevel_start, r))+ facet_wrap(~trait) "],
["finding-networks-from-patch-data-1.html", "Section 9 Finding networks from patch data 9.1 Read in patch data and overlaps 9.2 Overlaps in time and space 9.3 Association networks based on space-time overlap subsets 9.4 Get assortativity", " Section 9 Finding networks from patch data Workflow: Read in patch data, and link to individual trait data. Remove individuals with traits within 1 SD of the population mean, and their patches. Find communities in the remaining patches. Filter communities for data quality, and calculate assortativity. Relate assortativity to flock attributes: waterlevel start flock size 9.1 Read in patch data and overlaps # to handle data library(readr) library(scales) library(tidyr); library(tibble) library(magrittr) library(dplyr) library(purrr) library(stringr) # to work with networks library(igraph) # to plot library(ggplot2) library(scico) # read in patch data data_patches &lt;- read_csv(&quot;data/data2018/data_2018_patch_summary_has_patches.csv&quot;) %&gt;% mutate(uid = as.character(seq_len(nrow(.)))) # read in overlap data data &lt;- read_csv(&quot;data/data2018/data_spatio_temporal_overlap_2018.csv&quot;) 9.1.1 Read in trait data # make nodes data -- this the individual identities # add individual data to patch data data_id &lt;- readxl::read_excel(&quot;data/data2018/Biometrics_2018-2019.xlsx&quot;) %&gt;% filter(str_detect(`TAG NR`, &quot;[a-zA-Z]+&quot;, negate = TRUE)) # a function for gizzard mass get_gizzard_mass &lt;- function(x, y) {-1.09 + (3.78*(x*y))} # add gizzard mass data_id &lt;- mutate(data_id, gizzard_mass = get_gizzard_mass(SH1, SW1)) # rename columns and drop ids without mass and gizzard mass data_id &lt;- data_id %&gt;% select(id = `TAG NR`, wing = WING, mass = MASS, gizzard_mass) %&gt;% distinct(id, .keep_all = TRUE) %&gt;% drop_na(gizzard_mass) # add some exploration scores and tag info data_behav &lt;- read_csv(&quot;data/data2018/2018-19-all_exploration_scores.csv&quot;) %&gt;% filter(Exp == &quot;F01&quot;) data_tag &lt;- read_csv(&quot;data/data2018/tag_info.csv&quot;) %&gt;% mutate(id = as.character(Toa_Tag)) # join all scores data_id &lt;- left_join(data_id, data_tag, by = c(&quot;id&quot;)) %&gt;% left_join(data_behav, by = &quot;FB&quot;) # remove ids with no exploration data_id &lt;- mutate(data_id, behav = Mean) %&gt;% # drop_na(behav) %&gt;% select(id, mass, gizzard_mass, behav) 9.1.2 Filter out small patches Each fix corresponds to 30s time. data_patches &lt;- filter(data_patches, nfixes &gt; 3) data &lt;- data %&gt;% filter(patch_i_unique_id %in% data_patches$uid, patch_j_unique_id %in% data_patches$uid) 9.2 Overlaps in time and space 9.2.1 When do overlaps happen? When, in terms of waterlevel and season, do most overlaps occur? # convert to character data &lt;- mutate_at(data, vars(contains(&quot;patch&quot;)), as.character) data &lt;- left_join(data, data_patches, by = c(&quot;patch_i_unique_id&quot; = &quot;uid&quot;)) %&gt;% left_join(data_patches, by = c(&quot;patch_j_unique_id&quot; = &quot;uid&quot;)) Count at rounded waterlevel, and tide number. # round waterlevel start data_overlap_waterlevel &lt;- data %&gt;% mutate(waterlevel_round = plyr::round_any(waterlevel_start.x, 20)) %&gt;% count(waterlevel_round) # plot ggplot(data_overlap_waterlevel)+ geom_col(aes(x = waterlevel_round, y = n), fill = &quot;grey&quot;, col = &quot;black&quot;, size = 0.2)+ theme_bw()+ labs(x = &quot;waterlevel (cm NAP)&quot;, y = &quot;# overlaps&quot;, caption = &quot;Waterlevel is waterlevel_start.x rounded to 20 cm. 15 tidal cycles is approx. 1 week.&quot;, title = &quot;# patch overlaps ~ waterlevel&quot;) ggsave(filename = &quot;figs/fig_overlaps_waterlevel.png&quot;, dpi = 300, height = 4, width = 5) 9.2.2 Where do overlaps happen? Load the overlaps spatial object. overlaps &lt;- sf::st_read(&quot;data/data2018/spatials/patch_overlap_2018.gpkg&quot;) overlaps &lt;- filter(overlaps, uid %in% data_patches$uid) # get centroid of overlap overlaps &lt;- mutate(overlaps, centroid = sf::st_centroid(geom)) # get only uid - overlap id, and centroid overlaps &lt;- select(overlaps, uid, overlap_id, centroid) %&gt;% sf::st_drop_geometry() # get centroid as coords overlaps &lt;- bind_cols(overlaps, as_tibble(sf::st_coordinates(overlaps$centroid))) %&gt;% select(-centroid) # type to character overlaps &lt;- overlaps %&gt;% mutate_at(vars(matches(&quot;id&quot;)), as.character) Where on the mudflats do overlaps happen? # read griend griend &lt;- sf::st_read(&quot;map.osm&quot;, layer = &quot;multipolygons&quot;) %&gt;% filter(name %in% c(&quot;Richel&quot;, &quot;Griend&quot;)) %&gt;% sf::st_transform(32631) # count at x and y data_overlap_space &lt;- data %&gt;% left_join(overlaps, by = c(&quot;patch_i_unique_id&quot; = &quot;uid&quot;, &quot;patch_j_unique_id&quot; = &quot;overlap_id&quot;)) %&gt;% mutate(x_round = plyr::round_any(X, 100), y_round = plyr::round_any(Y, 100)) %&gt;% count(x_round, y_round) # plot ggplot(data_overlap_space)+ geom_tile(aes(x = x_round, y = y_round, fill = n))+ geom_sf(data = griend, fill = NA, size = 1, col = &quot;black&quot;)+ scale_fill_scico(trans = &quot;log10&quot;, palette = &quot;lajolla&quot;, values=c(0, 1), direction = 1, breaks = c(10^c(0:4)))+ labs(caption = &quot;X and Y are coords_start.x rounded to 200m.&quot;, title = &quot;# patch overlaps ~ space&quot;) ggsave(filename = &quot;figs/fig_overlaps_space.png&quot;, dpi = 300) 9.2.3 Where AND when do overlaps happen? # count at x and y data_overlap_spacetime &lt;- data %&gt;% left_join(overlaps, by = c(&quot;patch_i_unique_id&quot; = &quot;uid&quot;, &quot;patch_j_unique_id&quot; = &quot;overlap_id&quot;)) %&gt;% mutate(x_round = plyr::round_any(X, 200), y_round = plyr::round_any(Y, 200), waterlevel_round = plyr::round_any(waterlevel_start.x, 30)) %&gt;% count(x_round, y_round, waterlevel_round) # plot ggplot(data_overlap_spacetime)+ geom_tile(aes(x = x_round, y = y_round, fill = n))+ geom_sf(data = griend, fill = NA, size = 1, col = &quot;black&quot;)+ scale_fill_scico(trans = &quot;log10&quot;, palette = &quot;lajolla&quot;, values=c(0, 1), direction = 1, breaks = c(10^c(0:4)))+ facet_wrap(~waterlevel_round, labeller = label_both)+ theme_bw()+ labs(caption = &quot;X and Y are coords_start.x rounded to 200m.&quot;, title = &quot;# patch overlaps ~ space&quot;) ggsave(filename = &quot;figs/fig_overlaps_spacetime.png&quot;, dpi = 300) 9.3 Association networks based on space-time overlap subsets 9.3.1 Prepare subsets of data Round each overlap’s X, Y and waterlevel coordinates, and construct a network from all overlaps in the same subset of X, Y, time. # make data here data_overlap &lt;- data %&gt;% left_join(overlaps, by = c(&quot;patch_i_unique_id&quot; = &quot;uid&quot;, &quot;patch_j_unique_id&quot; = &quot;overlap_id&quot;)) %&gt;% mutate(x_round = plyr::round_any(X, 100), y_round = plyr::round_any(Y, 100), waterlevel_round = plyr::round_any(waterlevel_start.x, 30)) # select columns data_overlap &lt;- select(data_overlap, contains(&quot;round&quot;), id.x, id.y) # count interactions among unique pairs data_overlap &lt;- group_by(data_overlap, x_round, y_round, waterlevel_round) %&gt;% count(id.x, id.y) # nest in prep for making a network data_overlap &lt;- nest(data_overlap) # remove cases where there are fewer than 3 rows edge_df &lt;- filter(data_overlap, map_lgl(data, function(x) { nrow(x) &gt;= 3 })) 9.3.2 Get node data # expand nodes df to match edges data node_df &lt;- summarise(edge_df, id = map(data, function(df) {union(df$id.x, df$id.y)})) %&gt;% mutate(id = map(id, function(df){ df &lt;- tibble(id = as.character(df)) %&gt;% left_join(data_id, by = &quot;id&quot;) # remove nodes with no attributes df &lt;- drop_na(df, gizzard_mass) })) 9.3.3 Make networks using the edgelist and nodelist joined together. # make network df network_df &lt;- left_join(edge_df, node_df) # remove edges with no node data network_df &lt;- filter(network_df, map_lgl(id, function(df) { nrow(df) &gt; 1 })) # make networks for each of the many subsets network_df &lt;- mutate(network_df, net = map2(data, id, function(e_df, n_df) { # exclude edges with no vertex attributes e_df &lt;- filter(e_df, id.x %in% n_df$id, id.y %in% n_df$id) igraph::graph_from_data_frame(d = e_df, vertices = n_df, directed = F)} )) 9.3.4 Remove networks with few edges # count edges network_df &lt;- mutate(network_df, n_edges = map_int(net, function(n){ E(n) %&gt;% length() })) %&gt;% ungroup() # remove bad graphs, ie, with fewer than 3 edges # this is both for quality and practical reasons min_edges &lt;- 1 min_id &lt;- 2 network_df &lt;- filter(network_df, n_edges &gt;= min_edges) 9.4 Get assortativity 9.4.1 Get assortativity on node attributes # get weighred assortativity by gizzard mass network_df &lt;- mutate(network_df, assort_g_mass = unlist(purrr::map(net, function(n){ # adj &lt;- igraph::as_adjacency_matrix(n, # sparse = FALSE, # attr = &quot;n&quot;) assortativity.nominal(n, types = V(n)$gizzard_mass) }))) 9.4.2 Mean assortativity in space and time assort_df &lt;- network_df %&gt;% group_by(x_round, y_round, waterlevel_round) %&gt;% summarise(med_assort = median(assort_g_mass)) # plot ggplot(assort_df)+ geom_tile(aes(x = x_round, y = y_round, fill = med_assort))+ geom_sf(data = griend, fill = NA, size = 1, col = &quot;black&quot;)+ scale_fill_gradient2(low = &quot;blue&quot;, high = &quot;red&quot;, mid = &quot;grey90&quot;, limits = c(-1,1))+ facet_wrap(~ waterlevel_round, labeller = label_both)+ theme_bw()+ labs(caption = &quot;X and Y are coords_start.x rounded to 200m.&quot;, title = &quot;median assortativity in space and time&quot;) ggsave(filename = &quot;figs/fig_overlaps_spacetime.png&quot;, dpi = 300) "],
["pairwise-associations-in-relation-to-traits.html", "Section 10 Pairwise associations in relation to traits 10.1 Read in patch data and overlaps 10.2 Statistical modelling of number of associations 10.3 Statistical modelling of spatial overlap 10.4 Statistical modelling of temporal overlap 10.5 Save data to file for network analyses", " Section 10 Pairwise associations in relation to traits 10.1 Read in patch data and overlaps # to handle data library(readr) library(scales) library(tidyr); library(tibble) library(magrittr) library(dplyr) library(purrr) library(stringr) library(forcats) # to work with networks library(igraph) # to plot library(ggplot2) library(scico) # functions ci &lt;- function(x){qnorm(0.975)*sd(x, na.rm = T)/sqrt((length(x)))} # read in patch data data_patches &lt;- read_csv(&quot;data/data2018/data_2018_patch_summary_has_patches.csv&quot;) %&gt;% mutate(uid = as.character(1:nrow(.))) # read in overlap data data &lt;- read_csv(&quot;data/data2018/data_spatio_temporal_overlap_2018.csv&quot;) 10.1.1 Read in trait data # make nodes data -- this the individual identities # add individual data to patch data data_id &lt;- readxl::read_excel(&quot;data/data2018/Biometrics_2018-2019.xlsx&quot;) %&gt;% filter(str_detect(`TAG NR`, &quot;[a-zA-Z]+&quot;, negate = TRUE)) # a function for gizzard mass get_gizzard_mass &lt;- function(x, y) {-1.09 + (3.78*(x*y))} # add gizzard mass data_id &lt;- mutate(data_id, gizzard_mass = get_gizzard_mass(SH1, SW1)) # rename columns and drop ids without mass and gizzard mass data_id &lt;- data_id %&gt;% select(id = `TAG NR`, wing = WING, mass = MASS, gizzard_mass) %&gt;% distinct(id, .keep_all = TRUE) %&gt;% drop_na(gizzard_mass) # add some exploration scores and tag info data_behav &lt;- read_csv(&quot;data/data2018/2018-19-all_exploration_scores.csv&quot;) %&gt;% filter(Exp == &quot;F01&quot;) data_tag &lt;- read_csv(&quot;data/data2018/tag_info.csv&quot;) %&gt;% mutate(id = as.character(Toa_Tag)) # join all scores data_id &lt;- left_join(data_id, data_tag, by = c(&quot;id&quot;)) %&gt;% left_join(data_behav, by = &quot;FB&quot;) # remove ids with no exploration data_id &lt;- mutate(data_id, behav = Mean) %&gt;% # drop_na(behav) %&gt;% select(id, mass, gizzard_mass, behav) 10.1.2 What is the empirical distribution of differences in gizzard mass? 10.1.3 Filter out small patches Each fix corresponds to 30s time. data_patches &lt;- filter(data_patches, nfixes &gt; 3) data &lt;- data %&gt;% filter(patch_i_unique_id %in% data_patches$uid, patch_j_unique_id %in% data_patches$uid) 10.1.4 Link patches with overlaps # convert to character data &lt;- mutate_at(data, vars(contains(&quot;patch&quot;)), as.character) data &lt;- left_join(data, data_patches, by = c(&quot;patch_i_unique_id&quot; = &quot;uid&quot;)) %&gt;% left_join(data_patches, by = c(&quot;patch_j_unique_id&quot; = &quot;uid&quot;)) 10.1.5 Count pairwise overlaps # first clip data into 3 sections with lims at 0, 55 data_summary &lt;- data %&gt;% mutate(tide_number = tide_number.x, waterlevel = waterlevel_start.x, tide_stage = case_when(waterlevel &lt;= 0 ~ &quot;low&quot;, # between(waterlevel, 0, 55) ~ &quot;medium&quot;, waterlevel &gt; 0 ~ &quot;high&quot;, T ~ NA_character_)) %&gt;% # count as well as add strength in terms of space and time group_by(id.x, id.y, tide_number, tide_stage) %&gt;% summarise(n_associations = length(spatial_overlap_area), tot_overlap_space = sum(spatial_overlap_area), tot_overlap_time = sum(temporal_overlap_seconds)) %&gt;% ungroup() # link trait values data_summary &lt;- mutate_at(data_summary, vars(matches(&quot;id&quot;)), as.character) data_summary &lt;- left_join(data_summary, data_id, by = c(&quot;id.x&quot; = &quot;id&quot;)) %&gt;% left_join(data_id, by = c(&quot;id.y&quot; = &quot;id&quot;)) # get difference in traits data_summary &lt;- data_summary %&gt;% mutate(diff_gizzard = abs(gizzard_mass.x - gizzard_mass.y), diff_gizzard_round = plyr::round_any(diff_gizzard, 0.5), diff_behav = abs(behav.x - behav.y), tide_stage = forcats::as_factor(tide_stage), tide_stage = fct_relevel(tide_stage, &quot;low&quot;, &quot;high&quot;)) %&gt;% drop_na(diff_gizzard) 10.1.6 Assign unique pair ids # count the number of data in each class data_summary %&gt;% count(diff_gizzard_round) # count the difference in gizzards per pairwise association data_summary &lt;- data_summary %&gt;% group_by(id.x, id.y) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(uid_pair = seq_len(nrow(.))) # then unnest data_summary &lt;- unnest(data_summary, col = &quot;data&quot;) 10.1.7 Subsample for even coverage Small gizzard mass differences are over-represented as expected from a population with normally distributed traits. This requires subsampling based on difference bins. # filter for differences above 5 and subsample as many # as supported data_subsample &lt;- data_summary %&gt;% filter(diff_gizzard_round &lt;= 3) %&gt;% group_by(tide_stage, diff_gizzard_round) %&gt;% nest() %&gt;% mutate(data = map(data, function(x) x[1:1480,])) # this is hardcoded # unnest data_subsample &lt;- unnest(data_subsample, cols = &quot;data&quot;) 10.2 Statistical modelling of number of associations 10.2.1 Fit a GLMM for number of associations Difference in gizzard mass and behaviour score are predictors. # run a single canonical model library(lmerTest) model_n_assoc_full &lt;- glmer(n_associations ~ diff_gizzard * tide_stage + diff_behav * tide_stage + (1|tide_number) + (1|uid_pair), data = data_subsample, family = &quot;poisson&quot;) # run an alternative model with only behaviour model_n_assoc_behav_only &lt;- glmer(n_associations ~ diff_behav * tide_stage + (1|tide_number) + (1|uid_pair), data = data_subsample, family = &quot;poisson&quot;) # an alternative model with only gizzard mass model_n_assoc_gizzard_only &lt;- glmer(n_associations ~ diff_gizzard * tide_stage + (1|tide_number) + (1|uid_pair), data = data_subsample, family = &quot;poisson&quot;) 10.2.2 AIC comparison Compare the AIC weights after looking at the model summaries. summary(model_n_assoc_full) summary(model_n_assoc_behav_only) summary(model_n_assoc_gizzard_only) It seems that differences in behaviour as well as gizzard mass are each sufficient sufficient to explain the observed number of overlaps. Examine the AIC scores. # get aic difference map_dbl(list(behaviour_only = model_n_assoc_behav_only, gizzard_only = model_n_assoc_gizzard_only, gizzard_and_behav = model_n_assoc_full), AIC) With similar AIC scores and R2 values, we select the reduced model by the parsimony principle, and write the model output to file. model_output = capture.output(map(list(behaviour_only = model_n_assoc_behav_only, gizzard_only = model_n_assoc_gizzard_only, gizzard_and_behav = model_n_assoc_full), summary)) # save it to file write_lines(x = model_output, path = &quot;results/model_n_assoc.txt&quot;) Read in the model and print summary. cat(read_lines(&quot;results/model_n_assoc.txt&quot;), sep = &quot;\\n&quot;) 10.3 Statistical modelling of spatial overlap 10.3.1 GLMM for spatial overlap # run a model with overlap as the response library(lmerTest) model_sp_overlap_full &lt;- lmer(tot_overlap_space ~ diff_gizzard * tide_stage + diff_behav * tide_stage + (1|tide_number) + (1|uid_pair), data = data_subsample) # run an alternative model with only behaviour model_sp_overlap_behav_only &lt;- lmer(tot_overlap_space ~ diff_behav * tide_stage + (1|tide_number) + (1|uid_pair), data = data_subsample) # an alternative model with only gizzard mass model_sp_overlap_gizzard_only &lt;- lmer(tot_overlap_space ~ diff_gizzard * tide_stage + (1|tide_number) + (1|uid_pair), data = data_subsample) 10.3.2 AIC comparison Compare the AIC weights after looking at the model summaries. summary(model_sp_overlap_full) summary(model_sp_overlap_behav_only) summary(model_sp_overlap_gizzard_only) It seems that differences in behaviour are sufficient in this case to explain the observed extent of overlaps. Examine the AIC scores. # get aic difference map_dbl(list(behaviour_only = model_sp_overlap_behav_only, gizzard_only = model_sp_overlap_gizzard_only, gizzard_and_behav = model_sp_overlap_full), AIC) model_output = capture.output(map(list(behaviour_only = model_sp_overlap_behav_only, gizzard_only = model_sp_overlap_gizzard_only, gizzard_and_behav = model_sp_overlap_full), summary)) # save it to file write_lines(x = model_output, path = &quot;results/model_sp_overlap.txt&quot;) Read in the model summary and write. cat(read_lines(&quot;results/model_sp_overlap.txt&quot;), sep = &quot;\\n&quot;) 10.4 Statistical modelling of temporal overlap 10.4.1 GLMM for temporal overlap # run a model with overlap as the response library(lmerTest) model_time_overlap_full &lt;- lmer(tot_overlap_time ~ diff_gizzard * tide_stage + diff_behav * tide_stage + (1|tide_number) + (1|uid_pair), data = data_subsample) # run an alternative model with only behaviour model_time_overlap_behav_only &lt;- lmer(tot_overlap_time ~ diff_behav * tide_stage + (1|tide_number) + (1|uid_pair), data = data_subsample) # only gizzard model_time_overlap_gizzard_only &lt;- lmer(tot_overlap_time ~ diff_gizzard * tide_stage + (1|tide_number) + (1|uid_pair), data = data_subsample) 10.4.2 AIC and R2 comparison Compare the AIC weights after looking at the model summaries. summary(model_time_overlap_full) summary(model_time_overlap_reduced) Temporal overlap cannot be explained by the difference in gizzard mass or the difference in behaviour. This is an interesting pattern when taken together with the positive effect of behavioural difference on spatial overlap. # get aic difference AIC(model_time_overlap_full) - AIC(model_time_overlap_reduced) # get R2 MuMIn::r.squaredGLMM(model_time_overlap_reduced) 10.5 Save data to file for network analyses write_csv(data_summary, path = &quot;data/data_pairwise_comparisons.csv&quot;) "],
["bathymetry-of-the-griend-mudflats.html", "Section 11 Bathymetry of the Griend mudflats 11.1 Get data and plot basic trend 11.2 Plot as 3D maps", " Section 11 Bathymetry of the Griend mudflats 11.1 Get data and plot basic trend # load libs to read bathymetry library(raster) library(rayshader) library(sf) # data libs library(data.table) library(purrr) library(stringr) # plot libs library(ggplot2) library(ggthemes) # load bathymetry and subset data &lt;- raster(&quot;data/bathymetry_waddenSea_2015.tif&quot;) griend &lt;- st_read(&quot;griend_polygon/griend_polygon.shp&quot;) %&gt;% st_buffer(10.5e3) # assign utm 31 crs and subset crs(data) &lt;- unlist(st_crs(griend)[2]) data &lt;- crop(data, as(griend, &quot;Spatial&quot;)) data &lt;- aggregate(data, fact = 2) # remove 20m outliers #data[data &gt; 500] &lt;- NA data_m &lt;- raster_to_matrix(data) # get quantiles of the matrix data_q &lt;- quantile(data_m, na.rm = T, probs = 1:1000/1000) # read in waterlevel data waterlevel &lt;- fread(&quot;data/data2018/waterlevelWestTerschelling.csv&quot;, sep = &quot;;&quot;) # select useful columns and rename waterlevel &lt;- waterlevel[,.(WAARNEMINGDATUM, WAARNEMINGTIJD, NUMERIEKEWAARDE)] setnames(waterlevel, c(&quot;date&quot;, &quot;time&quot;, &quot;level&quot;)) # make a single POSIXct column of datetime waterlevel[,dateTime := as.POSIXct(paste(date, time, sep = &quot; &quot;), format = &quot;%d-%m-%Y %H:%M:%S&quot;, tz = &quot;CET&quot;)] waterlevel &lt;- setDT(distinct(setDF(waterlevel), dateTime, .keep_all = TRUE)) # plot waterlevel quantiles with data from west terschelling waterlimits &lt;- range(waterlevel$level) fig_waterlevel_area &lt;- ggplot()+ geom_line(aes(x = 1:1000/1000, y = data_q))+ geom_hline(yintercept = waterlimits, col = &quot;blue&quot;, lty = 2)+ geom_hline(yintercept = 0, lty = 2, col = &quot;red&quot;)+ scale_x_continuous(labels = scales::percent)+ theme_bw()+ coord_flip(xlim = c(0.25, 1.01), ylim = c(-250, 250))+ labs(x = &quot;% area covered&quot;, y = &quot;waterlevel (cm over NAP)&quot;) ggsave(fig_waterlevel_area, filename = &quot;figs/fig_waterlevel_area.png&quot;, dpi = 300, height = 4, width = 6); dev.off() 11.2 Plot as 3D maps # make sequence waterdepth &lt;- seq(waterlimits[1], waterlimits[2], length.out = 30) waterdepth &lt;- c(waterdepth, rev(waterdepth)) # make visualisation for(i in 1:length(waterdepth)) { data_m %&gt;% sphere_shade(texture = &quot;imhof1&quot;, zscale = 50) %&gt;% # add_water(detect_water(data_m, zscale = 100, # max_height = waterdepth[i],cutoff = 0.1), # color = &quot;desert&quot;) %&gt;% add_shadow(hillshade = data_m) %&gt;% plot_3d(data_m, zscale = 75, wateralpha = 0.6, solid =F, shadow=F, water = TRUE, waterdepth = waterdepth[i], watercolor = &quot;dodgerblue1&quot;, phi = 90, theta = 0, zoom = 0.75, windowsize = c(1000, 800), background = &quot;black&quot;, calculate_normals = F) render_label(data_m, x = 500, y = 500, z = 500, text = glue::glue(&#39;waterdepth = {round(waterdepth[i])} cm&#39;), freetype = F, textcolor = &quot;black&quot;) rgl::snapshot3d(paste0(&quot;figs/tide_rise/fig&quot;,str_pad(i, 2, pad = &quot;0&quot;),&quot;.png&quot;)) rgl::rgl.close() } library(magick) list.files(path = &quot;figs/tide_rise/&quot;, pattern = &quot;*.png&quot;, full.names = T) %&gt;% map(image_read) %&gt;% # reads each path file image_join() %&gt;% # joins image image_animate(fps=2) %&gt;% # animates, can opt for number of loops image_write(&quot;figs/fig_tide_rise_anim.gif&quot;) # write to current dir if (knitr:::is_latex_output()) { } else { knitr::include_graphics(&quot;figs/fig_tide_rise_anim.gif&quot;) } (#fig:include_gif)Waterlevel at West Terschelling and effect on coverage of mudflats around Griend. Bracis, Chloe, Keith L. Bildstein, and Thomas Mueller. 2018. “Revisitation Analysis Uncovers Spatio-Temporal Patterns in Animal Movement Data.” Ecography 41 (11): 1801–11. https://doi.org/10.1111/ecog.03618. Troy D. Hill, Shimon C. Anisfeld. 2014. VulnToolkit: R for Coastal Vulnerability Analysis. New Haven, CT: Yale School of Forestry; Environmental Studies. https://github.com/troyhill/VulnToolkit. "]
]
